/* 
  CharmBH - DataManager group (one member per PE)
  Each PE does the following:

  1. Loads its shares of particles from input file.
  2. Decomposes the particles onto tree pieces.
  3. After decomposition, accepts particles from all
     tree pieces hosted on PE to build combined tree
     from them.
  4. Initiates traversals for its tree pieces after 
     on-PE tree is built.
  5. Maintains a cache of remote data requested by
     its tree pieces.
  6. Serves local data requests generated by remote PEs.
  7. Advances particles when traversals have finished
     on all PEs.

*/

#define TB_DEBUG 

#include "DataManager.h"
#include "Reduction.h"
#include "defines.h"
#include "Messages.h"
#include "Parameters.h"

#include "Worker.h"
#include "TreePiece.h"

#include "Request.h"
#include "defaults.h"

#include "TreeMerger.h"

#include <fstream>
#include <iostream>
#include <sstream>

using namespace std;
extern CProxy_TreePiece treePieceProxy;
extern CProxy_TreeMerger treeMergerProxy;
extern CProxy_Main mainProxy;
extern Parameters globalParams;

extern CProxy_MeshStreamer<NodeRequest> combinerProxy;

#define RRDEBUG 
//#define RRDEBUG if(CkMyPe() == 1) CkPrintf
//#define RRDEBUG CkPrintf

/*
  This function is called during tree building. When a 
  PE requests the data for a remote node, it receives only
  the type, moments and bounding box for it (since these
  are properties that cannot be calculated by this PE, given
  that it has no particles under the requested remote node).
*/
void copyMomentsToNode(Node<ForceData> *node, const MomentsExchangeStruct &mes){
  CkAssert(node->getKey() == mes.key);

  node->data.moments = mes.moments;
  node->data.box = mes.box;
  NodeType type = mes.type;
  node->setType(Node<ForceData>::makeRemote(type));

}

DataManager::DataManager() : 
  iteration(0),
  prevIterationStart(0.0),
  root(NULL)
{
  numPesPerNode = CkNumPes()/CkNumNodes();
  numLocalUsefulTreePieces = 0;
  doSkipDecomposition = false;
  init();

  //tpArray = CkArrayID::CkLocalBranch(treePieceProxy.ckGetArrayID());
}

/*
  Load your share of the particles from the input
  file. 
*/
void DataManager::loadParticles(CkCallback &cb){

  // set up my own proxy, local combiner, TP ckarray
  myProxy = CProxy_DataManager(thisgroup);
  combiner = ((MeshStreamer<NodeRequest> *)CkLocalBranch(combinerProxy));
  tpArray = treePieceProxy.ckGetArrayID().ckLocalBranch();

  numRankBits = LOG_BRANCH_FACTOR;

  const char *fname = globalParams.filename;
  int npart = globalParams.numParticles;

  std::ifstream partFile;
  partFile.open(fname, ios::in | ios::binary);
  CkAssert(partFile.is_open());

  /*
    Calculate your share of input particles,
    and the offset into the input file from
    where you are to begin reading.
  */

  streamoff offset = 0; 
  int myid = CkMyPe();
  int npes = CkNumPes();

  /*
    Calculate your share of input particles,
    and the offset into the input file from
    where you are to begin reading.
  */
  int avgParticlesPerPE = npart/npes;
  int rem = npart-npes*avgParticlesPerPE;
  if(myid < rem){
    avgParticlesPerPE++;
    offset = myid*avgParticlesPerPE;
  }
  else{
    offset = myid*avgParticlesPerPE+rem;
  }
  myNumParticles = avgParticlesPerPE;
  offset *= SIZE_PER_PARTICLE;
  offset += PREAMBLE_SIZE;

  /*
    Reserve enough space for your particles.
  */
  myParticles.reserve(myNumParticles);
  myParticles.length() = myNumParticles;

  /*
    Seek to correct starting position in input file.
  */
  partFile.clear();
  partFile.seekg(offset,ios::beg);
  if(partFile.fail()){
    std::ostringstream oss;
    oss << "couldn't seek to position " << offset << " on PE " << CkMyPe() << " position " << partFile.tellg() << endl;
    CkAbort(oss.str().c_str());
  }
  unsigned int numParticlesDone = 0;

  BoundingBox myBox;

  /*
    Read particles.
  */
  Real tmp[REALS_PER_PARTICLE];
  myBox.energy = 0.0;
  /*
    Read particles.
  */
  while(numParticlesDone < myNumParticles && !partFile.eof()){
    partFile.read((char *)tmp, SIZE_PER_PARTICLE);
    Particle &p = myParticles[numParticlesDone];
    p.position.x = tmp[0];
    p.position.y = tmp[1];
    p.position.z = tmp[2];
    p.velocity.x = tmp[3];
    p.velocity.y = tmp[4];
    p.velocity.z = tmp[5];
    p.mass = tmp[6];

    // these will be calculated during the course of the iteration
    p.acceleration = Vector3D<Real>(0.0);
    p.potential = 0.0;

    // in order to find the bounding box of your particles
    myBox.grow(p.position);
    // accumulate KE for this time period
    myBox.energy += p.mass*p.velocity.lengthSquared();

    numParticlesDone++;
  }
  myBox.energy /= 2.0;

  CkAssert(numParticlesDone == myNumParticles);
  myBox.numParticles = myNumParticles;

  partFile.close();

  /*
    Each PE contributes the bounding box of its particles to a reduction.
    At the root of the reduction (in Main.cpp) we obtain the bounding box
    of all loaded particles. This bounding box is required in order to
    obtain the key for each particle.
  */
  contribute(sizeof(BoundingBox),&myBox,BoundingBoxGrowReductionType,cb);
}

/*
  Obtain a 64-bit key for each particle. This gives us a cheap way to
  arrange particles within the Barnes-Hut tree. The hash below is a 
  simple interleaving of the bits of each component of a particle's
  position vector.

  The idea is to locate each particle within a three-dimensional grid which
  has BOXES_PER_DIM points in each dimension, and whose extents are
  the same as those of the simulation universe. Thus, for each particle, 
  we use its position coordinates to obtain three integer values telling which
  of those grid points it falls on. The width of each integer is BOXES_PER_DIM,
  and by interleaving the bits of the three integers, we can tell the position
  of the particle in the Barnes-Hut tree.
*/
void DataManager::hashParticleCoordinates(OrientedBox<double> &universe){
  Key prepend;
  prepend = 1L;
  prepend <<= (TREE_KEY_BITS-1);

  Real xsz = universe.greater_corner.x-universe.lesser_corner.x;
  Real ysz = universe.greater_corner.y-universe.lesser_corner.y;
  Real zsz = universe.greater_corner.z-universe.lesser_corner.z;

  for(unsigned int i = 0; i < myNumParticles; i++){
    Particle *p = &(myParticles[i]);
    // Obtain the integer grid points on which the particle falls in each dimension
    Key xint = ((Key) (((p->position.x-universe.lesser_corner.x)*(BOXES_PER_DIM*1.0))/xsz)); 
    Key yint = ((Key) (((p->position.y-universe.lesser_corner.y)*(BOXES_PER_DIM*1.0))/ysz)); 
    Key zint = ((Key) (((p->position.z-universe.lesser_corner.z)*(BOXES_PER_DIM*1.0))/zsz)); 

    // Interleave bits
    Key mask = Key(0x1);
    Key k = Key(0x0);
    int shiftBy = 0;
    for(int j = 0; j < BITS_PER_DIM; j++){
      k |= ((zint & mask) <<  shiftBy);
      k |= ((yint & mask) << (shiftBy+1));
      k |= ((xint & mask) << (shiftBy+2));
      mask <<= 1;
      // minus 1 because mask itself has shifted
      // left by one position
      shiftBy += (NDIMS-1);
    }
    // Prepend the key with a '1' bit.
    k |= prepend; 
    myParticles[i].key = k;
  }
}

/*
  Use the keys of the particles (as calculated above) 
  to decompose them onto tree pieces. Each tree piece
  gets a subvolume of the entire universe. This subvolume
  is represented in the tree by a particular node (internal
  or leaf). All particles within this subvolume fall beneath
  the node, so that the node's key is the common prefix of 
  all particles enclosed by the subvolume.

  Through the decomposition, we obtain several subsets of 
  particles, each of which constitutes a subvolume of the 
  universe. This subvolume corresponds to a tree piece, and
  the number of particles within it is guaranteed to be less
  than a user-specified threshold (ppc/particles per chare).

  The decomposition procedure is iterative, similar to
  histogram sort. In each iteration, a master PE (0) maintains
  a list of "active" nodes (leaves) which are to be partitioned.
  The keys of these nodes are broadcast to the workers (PE 0 is 
  both master and worker). The workers split the corresponding
  nodes into children and partition the particles previously in
  the active nodes to the appropriate children. They then contribute
  the number of particles they hold under each child to a reduction.
  The master uses the result of this reduction to find which nodes
  are to be split further: These are made active leaves for the 
  next iteration; this continues until there are no remaining active
  leaves.

  Decomposition begins at the root of the global tree, which 
  represents the bounding box enclosing all particles in the 
  simulated universe.
*/
void DataManager::decompose(BoundingBox &universe){

  // Obtain key for each particle given the extents of the universe.
  hashParticleCoordinates(universe.box);
  // Sort particles so that decomposition can be done in-place.
  myParticles.quickSort();

  if(CkMyPe()==0){
    // Check whether total energy remains (about) constant
    if(iteration == 1){
      // save this value so that we can compare
      // against it in future iterations. 
      compareEnergy = universe.energy;
    }
    else if(iteration > 1){
      Real deltaE = compareEnergy-universe.energy;
      if(deltaE < 0) deltaE = -deltaE;
      // The energy should grow in magnitude
      // by less than a tenth of one per cent.
      CkAssert(deltaE/compareEnergy < 0.001);
      CkPrintf("(%d) iteration %d deltaE/E %f\n", CkMyPe(), iteration, deltaE/compareEnergy);
    }

    // Print statistics
    float memMB = (1.0*CmiMemoryUsage())/(1<<20);
    ostringstream oss; 
    CkPrintf("(%d) prev time %g s\n", CkMyPe(), CmiWallTimer()-prevIterationStart);
    CkPrintf("(%d) mem %.2f MB\n", CkMyPe(), memMB);
    CkPrintf("(%d) iteration %d univ %f %f %f %f %f %f energy %f\n", 
        CkMyPe(),
        iteration,
        universe.box.lesser_corner.x,
        universe.box.lesser_corner.y,
        universe.box.lesser_corner.z,
        universe.box.greater_corner.x,
        universe.box.greater_corner.y,
        universe.box.greater_corner.z,
        universe.energy);


    prevIterationStart = CkWallTimer();
  }

  if(doSkipDecomposition){
    skipFlushParticles();
  }
  else{
    /* 
       We increase the number of required tree pieces
       as the decomposition iterations proceed. Begin
       with a single tree piece holding all the particles.
     */
    numTreePieces = 1;

    // How many particles do I hold?
    initHistogramParticles();
    // Send this count to the master PE
    sendHistogram();
  }
}

/*
  Find out how many particles held by this PE
  fall under the root node (i.e. contribute the
  number of particles held by this PE)
*/
void DataManager::initHistogramParticles(){
  int rootDepth = 0;
  
  root = new Node<ForceData>(Key(1),
                         rootDepth,
                         myParticles.getVec(),
                         myNumParticles);
  /*
    The activeBins data structure keeps track of the
    active leaves (bins) iteration after iteration. 
    We begin with the root as the only active leaf. 
    When a worker is told by the master to split an
    active leaf, it partitions its particles among
    its children and contributes the number of particles
    held by each child to a reduction.
  */
  activeBins.addNewNode(root);

  // don't access myParticles through ckvec after this
  // anyway. these must be reset before this DM starts
  // to receive submitted particles from TPs placed on it
  myNumParticles = 0;
  myParticles.length() = 0;
}

/*
  Send counts of the particles held by this PE that belong
  to each of the newly created children (i.e. children 
  of active leaves that are being partitioned) 
*/
void DataManager::sendHistogram(){
  CkCallback cb(CkIndex_DataManager::receiveHistogram(NULL),0,this->thisgroup);
  contribute(sizeof(int)*activeBins.getNumCounts(),activeBins.getCounts(),CkReduction::sum_int,cb);
  activeBins.reset();
}

// executed on PE 0
void DataManager::receiveHistogram(CkReductionMsg *msg){
  int numRecvdBins = msg->getSize()/sizeof(int);
  int *descriptors = (int *)msg->getData();
  CkVec<int> binsToRefine;

  /* 
    Reserve enough space so that all active leaves 
    can be partitioned, if necessary
  */
  binsToRefine.reserve(numRecvdBins);
  binsToRefine.length() = 0;

  int particlesHistogrammed = 0;
  /*
    The newly created children of the active leaves
    are now the active leaves themselves. 
  */
  CkVec<pair<Node<ForceData>*,bool> > *active = activeBins.getActive();
  CkAssert(numRecvdBins == active->length());

  // Check which new active leaves need to be partitioned
  for(int i = 0; i < numRecvdBins; i++){
    if(descriptors[i] > (Real)(DECOMP_TOLERANCE*globalParams.ppc)){
      // Need to refine this leaf (partition)
      binsToRefine.push_back(i);
      // By refining this node, we will remove one tree piece
      // and add BRANCH_FACTOR in its place.
      numTreePieces += (BRANCH_FACTOR-1);
      // Fail if not enough tree pieces are provided
      if(numTreePieces > globalParams.numTreePieces){
        CkPrintf("have %d treepieces need %d\n",globalParams.numTreePieces,numTreePieces);
        CkAbort("Need more tree pieces!\n");
      }
    }

    particlesHistogrammed += descriptors[i];
  }

  // Do any active leaves need to be partitioned?
  if(binsToRefine.size()) {
    // Yes, tell workers which ones  
    myProxy.receiveSplitters(binsToRefine);
    decompIterations++;
  }
  else{
    // No more leaves to refine; send out particles to tree pieces
    CkPrintf("[0] decomp done after %d iterations used treepieces %d\n", decompIterations, numTreePieces);
    decompIterations = 0;
    /*
      Tell all PEs that there are no remaining active leaves,
      i.e. we have obtained a partitioning of particles on to
      tree pieces such that each tree piece gets no more than 
      a threshold (ppc) number of particles.
    */
    myProxy.sendParticles(numTreePieces);
  }

  delete msg;
}

void DataManager::skipFlushParticles(){
  //CkPrintf("[%d] skip decomposition, flush particles %d treepieces %d\n", CkMyPe(), myNumParticles, numTreePieces);
  int pstart = 0;
  int pend = myNumParticles;
  //CkPrintf("(%d) SKIP TP 0 node %llu\n", CkMyPe(), treePieceRoots[0]);
  for(int i = 1; i < numTreePieces; i++){
    Key check = treePieceRoots[i];
    //CkPrintf("(%d) SKIP TP %d node %llu\n", CkMyPe(), i, treePieceRoots[i]);
    int prev_start = pstart;
    pstart = binary_search_ge<Key,Particle>(check,myParticles.getVec(),pstart,pend); 

    int prev_np = pstart-prev_start;

    sendParticleMsg(i-1, myParticles.getVec()+prev_start, prev_np);
  }
  sendParticleMsg(numTreePieces-1,myParticles.getVec()+pstart,pend-pstart);
}

/*
  As a result of the decomposition procedure, the
  particles held by each PE have been partitioned 
  among the tree pieces. It is now time to send these
  particles to their respective tree pieces.

  We do this by traversing the tree that
  was constructed during the decompostion by the 
  activeBins data structure.
*/
void DataManager::flushParticles(){
  treePieceRoots.length() = 0;
  treePieceRoots.resize(numTreePieces);
  int numUsefulTreePieces = flushAndMark(root,0);
#if 0
  doneFlushParticles = true;
#endif
}

/*
  The master communicates the active leaves to split
  to the workers through this method. The activeBins
  data structure processes the list of leaves to refine,
  creates new children for them and obtains counts
  of the number of particles under each child. These
  children are then made the active leaves for the new
  iteration and the counts are contributed to a reduction
  in sendHistogram.
*/
void DataManager::receiveSplitters(CkVec<int> splitBins) {

  // Process bins to refine
  activeBins.processRefine(splitBins.getVec(), splitBins.size());

  // Send counts of particles in active leaves to master PE
  sendHistogram();
}

/*
  Send the particles that you are holding for tree piece 'tp' to it.
*/
void DataManager::sendParticlesToTreePiece(Node<ForceData> *nd, int tp) {
  CkAssert(nd->getNumChildren() == 0);
  int np = nd->getNumParticles();
  treePieceRoots[tp] = Node<ForceData>::getParticleLevelKey(nd);
  //CkPrintf("(%d) DECOMP TP %d node %llu\n", CkMyPe(), tp, nd->getKey());

  sendParticleMsg(tp,nd->getParticles(),np);
}

void DataManager::sendParticleMsg(int tp, Particle *p, int np){
  int bytesToSend = np*sizeof(Particle);
  int curoffset = 0;
  int curnum;
  int cursize;

  while(bytesToSend > 0){
    if(bytesToSend > globalParams.particleMsgMaxSize){
      cursize = globalParams.particleMsgMaxSize;
    }
    else{
      cursize = bytesToSend;
    }
    curnum = cursize/sizeof(Particle);
    cursize = curnum*sizeof(Particle);

    //CkPrintf("particlemsg num %d size %d\n", curnum, cursize);

    ParticleMsg *msg = new (curnum,0) ParticleMsg;
    memcpy(msg->part, p+curoffset, cursize);
    msg->numParticles = curnum;
    treePieceProxy[tp].receiveParticles(msg);

    bytesToSend -= cursize;
    curoffset += curnum;
  }

  CkAssert(bytesToSend == 0);
}

/*
  This method is invoked on workers by the master, telling
  them that it is now OK to send the particles they are
  holding to the tree pieces that they are meant for.

  It also communicates the ranges of particles held  by each
  tree piece to every PE. 
*/
void DataManager::sendParticles(int ntp){
  // Save tree piece particle ranges
  numTreePieces = ntp;
  // Flush particles to their owner tree pieces
  flushParticles();

#if 0
  // Obtain the number of tree pieces that are hosted on this PE
  senseTreePieces();
#endif
}

/*
  How many tree pieces are hosted on this PE? 
  This tells the PE how many tree pieces it should
  expect particle contributions from before starting
  to build the local tree.
*/
void DataManager::senseTreePieces(){
  localTreePieces.reset();
  CkLocMgr *mgr = treePieceProxy.ckLocMgr();
  mgr->iterate(localTreePieces);
#if 0
  numLocalTreePieces = localTreePieces.count;
#endif

#if 0
  /* 
    It could be that all tree pieces on this PE received their
    particles and submitted them to the DM before it received the 
    "sendParticles" message. If this is the case, proceed to tree building.
  */
#endif

#if 0
  CkAssert(doneFlushParticles);
#endif
#if 0
  if(submittedParticles.length() == numLocalTreePieces) processSubmittedParticles();
#endif
}

#if 0
/*
  This is a normal C++ function called by tree pieces hosted
  on this PE. Once a tree piece has received all the particles meant
  for it, it submits them to the DM. The DM, in turn, collects particles
  from all the tree pieces on its PE and constructs a local tree
  from them. All nodes and particles within this PE-level tree are
  then visible to all the tree pieces on the PE.
*/
void DataManager::submitParticles(CkVec<ParticleMsg*> *vec, int numParticles, TreePiece * tp){ 
  submittedParticles.push_back(TreePieceDescriptor(vec,numParticles,tp,tp->getIndex()));
  myNumParticles += numParticles;
  if(submittedParticles.length() == numLocalTreePieces && doneFlushParticles){
    processSubmittedParticles();
  }
}
#endif

/*
  Once the DM has collected particles from all the tree pieces hosted
  on this PE, it copies these particles into a contiguous buffer,
  sorts them and partitions them in-place in much the same way as 
  was done during domain decomposition. The result of this partitioning
  is a local tree whose leaves are buckets containing particles on
  the PE. The nodes of this tree are of six types:

  1. Bucket: leaf containing local particles
  2. EmptyBucket: an empty bucket
  3. Internal: a node whose every child is either Internal or Bucket or EmptyBucket

  4. RemoteBucket: leaf that is local to some other PE
  5. EmptyRemoteBucket: empty RemoteBucket
  6. Remote: a node whose entire subtree is non-local to this PE

  7. Boundary: a node that is none of the above, i.e. is "shared" between several
     PEs.
*/
void DataManager::processSubmittedParticles(){

  //CkPrintf("(%d) processSubmittedParticles\n", CkMyPe());
  /*
  CkPrintf("(%d) memcheck before processSubmittedParticles\n", CkMyPe());
  */

  // get the tree pieces (and their particles) on this PE
  senseTreePieces();

  // sort the local tree pieces by index (and hence lower/upper bound of particle range)
  localTreePieces.submittedParticles.quickSort();
  myNumParticles = localTreePieces.numParticles;
  
  myParticles.length() = 0;
  myParticles.resize(myNumParticles);

  int offset = 0;
  numLocalUsefulTreePieces = 0;
  for(int i = 0; i < localTreePieces.count; i++){
    TreePieceDescriptor &descr = localTreePieces.submittedParticles[i];
    //CkPrintf("(%d) tree piece %d submitted %d particles:\n", CkMyPe(), descr.index, descr.numParticles);
    
    if(descr.index < numTreePieces) numLocalUsefulTreePieces++;

    CkVec<ParticleMsg*> *vec = descr.vec;
    for(int j = 0; j < vec->length(); j++){
      ParticleMsg *msg = (*vec)[j];
      if(msg->numParticles > 0) memcpy(myParticles.getVec()+offset,msg->part,sizeof(Particle)*msg->numParticles);
      /*
      for(int k = offset; k < offset+msg->numParticles; k++){
        CkPrintf("(%d) particle %d key %lu\n", CkMyPe(), k, myParticles[k].key);
      }
      */
      offset += msg->numParticles;
      delete msg;
    }
  }

  // XXX can make this a number of smaller sorts
  myParticles.quickSort();

  // The first local TreePiece will always have buckets numbered from 0
  if(localTreePieces.count > 0) localTreePieces.submittedParticles[0].bucketStartIdx = 0;

  buildTree(root,0,myNumParticles,0,numLocalUsefulTreePieces);

  // Set the set of buckets assigned to each non-useful tree piece to empty:
#if 0
  for(int i = numLocalUsefulTreePieces; i < numLocalTreePieces; i++){
    TreePieceDescriptor &tp = submittedParticles[i];
    tp.bucketStartIdx = tp.bucketEndIdx = myBuckets.length(); 
  }
#endif
  doneTreeBuild = true;

#if 0
  buildTree();
  // makeMoments also sends out requests for moments 
  // of remote nodes
  makeMoments();
#endif

  /*
  // are all particles local to this PE? 
  if(root != NULL && root->allChildrenMomentsReady()){
    passMomentsUpward(root);
  }
  */
  /*
  CkPrintf("(%d) memcheck after processSubmittedParticles\n", CkMyPe());
  */

  if(CkMyPe() == 0){
    CkCallback cb(CkIndex_DataManager::processSubmittedParticles(),myProxy);
    combiner->associateCallback(cb,false);
  }
  combiner->enablePeriodicFlushing();
}

#if 0
void DataManager::buildTree(){

  int rootDepth = 0;
  root = new Node<ForceData>(Key(1),rootDepth,myParticles.getVec(),myNumParticles);
  root->setOwners(0,numTreePieces-1);
  nodeTable[Key(1)] = root;
  if(myNumParticles == 0){
    return;
  }

  OwnershipActiveBinInfo<ForceData> abi(keyRanges);
  abi.addNewNode(root);
  int numFatNodes = 1;

  int limit = ((Real)globalParams.ppb*BUCKET_TOLERANCE);

  CkVec<int> refines;

  while(numFatNodes > 0){
    abi.reset();
    refines.length() = 0;

    CkVec<std::pair<Node<ForceData>*,bool> > *active = abi.getActive();

    for(int i = 0; i < active->length(); i++){
      Node<ForceData> *node = (*active)[i].first;
      if(node->getNumParticles() > 0 && ((node->getOwnerEnd() > node->getOwnerStart()) || 
         (node->getNumParticles() > limit))){
        refines.push_back(i);
      }
    }

    abi.processRefine(refines.getVec(), refines.length());
    numFatNodes = abi.getNumCounts();
  }
  */

}
#endif

#if 0
void DataManager::makeMoments(){
  if(root == NULL) return;

  MomentsWorker mw(submittedParticles, nodeTable, localTPRoots, myBuckets);
  fillTrav.postorderTraversal(root,&mw);
}
#endif

Node<ForceData> *DataManager::lookupNode(Key k){
  map<Key,Node<ForceData>*>::iterator it;
  it = nodeTable.find(k);
  if(it == nodeTable.end()) return NULL;
  else return it->second;
}

void DataManager::requestMoments(Key k, int replyTo){
  pendingMoments[k].push_back(replyTo);
  TB_DEBUG("(%d) received requestMoments from pe %d node %lu\n", CkMyPe(), replyTo, k);

  if(doneTreeBuild){
    Node<ForceData> *node = lookupNode(k);
    CkAssert(node != NULL);
#if 0
    if(node == NULL){
      CkPrintf("(%d) recvd request from %d for moments %lu\n", CkMyPe(), replyTo, k);
      CkAbort("bad request\n");
    }
#endif

    bool ready = node->allChildrenMomentsReady();
    TB_DEBUG("(%d) ready %d node %lu\n", CkMyPe(), ready, k);

    if(ready) momentsResponseHelper(node);
  }
}

#if 0
void DataManager::flushMomentRequests(){
  map<Key,CkVec<int> >::iterator it;
  for(it = pendingMoments.begin(); it != pendingMoments.end();){
    Key k = it->first;
    Node<ForceData> *node = lookupNode(k);
    CkAssert(node != NULL);
    if(node->allChildrenMomentsReady()){
      CkVec<int> &requestors = it->second;
      respondToMomentsRequest(node,requestors);
      map<Key,CkVec<int> >::iterator kill = it;
      ++it;
      pendingMoments.erase(kill);
    }
    else{
      ++it;
    }
  }
}
#endif

void DataManager::respondToMomentsRequest(Node<ForceData> *node, CkVec<int> &replyTo){
  MomentsExchangeStruct moments = *node;
  for(int i = 0; i < replyTo.length(); i++){
    TB_DEBUG("(%d) responding to %d with node %lu\n", CkMyPe(), replyTo[i], node->getKey());
    CkEntryOptions opts;
    opts.setQueueing(CK_QUEUEING_IFIFO);
    opts.setPriority(RECV_MOMENTS_PRIORITY);
    myProxy[replyTo[i]].receiveMoments(moments, &opts);
  }
  replyTo.length() = 0;
}

void DataManager::receiveMoments(MomentsExchangeStruct moments){
  Node<ForceData> *node = lookupNode(moments.key);
  CkAssert(node != NULL);
  CkAssert(!treeMomentsReady);

  numMomentsReceived++;
  // update moments of leaf and pass these on 
  // to parent recursively; if there are requests
  // for these nodes, respond to them
  updateLeafMoments(node,moments);
}

void DataManager::updateLeafMoments(Node<ForceData> *node, MomentsExchangeStruct &data){
  copyMomentsToNode(node,data);
  TB_DEBUG("(%d) updateLeafMoments %lu\n", CkMyPe(), node->getKey());
  passMomentsUpward(node);
}

void DataManager::momentsResponseHelper(Node<ForceData> *node){
  map<Key,CkVec<int> >::iterator it = pendingMoments.find(node->getKey());
  if(it != pendingMoments.end()){
    CkVec<int> &requestors = it->second;
    respondToMomentsRequest(node,requestors);
    pendingMoments.erase(it);
  }
}

void DataManager::passMomentsUpward(Node<ForceData> *node){
  TB_DEBUG("(%d) passUp %lu\n", CkMyPe(), node->getKey());
  momentsResponseHelper(node);

  Node<ForceData> *parent = node->getParent();
  if(parent == NULL){
    CkAssert(node->getKey() == Key(1));
    treeReady();
  }else{
    parent->childMomentsReady();
    TB_DEBUG("[%d] passup children ready %d for node %lu\n", CkMyPe(), parent->getNumChildrenMomentsReady(), parent->getKey());
    if(parent->allChildrenMomentsReady()){
      parent->getMomentsFromChildren();
      passMomentsUpward(parent);
    }
  }
}

void DataManager::treeReady(){
#ifdef NODE_LEVEL_MERGE
  nodeLevelMerge();
#else
  CkAssert(numMomentsRequested == numMomentsReceived);
  treeMomentsReady = true;
  flushBufferedRemoteDataRequests();
  startTraversal();
#endif
}

void DataManager::nodeLevelMerge(){
  string name("unmerged");
  //doPrintTree(name);
  treeMergerProxy.ckLocalBranch()->submit(CkMyPe(),root);
}

void DataManager::doneNodeLevelMerge(PointerContainer container){
  Node<ForceData> *mergedRoot = (Node<ForceData> *) container.ptr;
  // register (i.e. fill into nodeTable)
  // nodes that are on paths to your useful 
  // tree pieces
  if(mergedRoot != root){
    delete root;
    root = mergedRoot;
  }

  //CkPrintf("DM %d register\n", CkMyPe());
  registerTopLevelNodes(root,0,numLocalUsefulTreePieces);
  treeMomentsReady = true;
  flushBufferedRemoteDataRequests();


  //CkPrintf("DM %d starttraversal\n", CkMyPe());
  startTraversal();

  //CkCallback cb(CkCallback::ckExit);
  //contribute(0,0,CkReduction::sum_int,cb);
}

void DataManager::registerTopLevelNodes(Node<ForceData> *node, int tpstart, int tpend){
  nodeTable[node->getKey()] = node;
  if(tpend <= tpstart){
    //CkPrintf("%d REGISTER %llu external\n", CkMyPe(), node->getKey());
    return;
  }
  else if(tpend-tpstart == 1 && (node->getOwnerEnd()-node->getOwnerStart()==1)){
    ostringstream buckets;
    TreePieceDescriptor &tp = localTreePieces.submittedParticles[tpstart];
    
    // if this node was a bucket and is being registered here,
    // it could be the merged replacement for a previous node.
    // therefore we must correct the pointer to this bucket in
    // myBuckets
    if(node->getType() == Bucket || node->getType() == EmptyBucket){
      // since this is the root of a tree piece (because registerTopLevelNodes
      // does not descend lower than the roots of tree pieces) and a bucket
      // it must be the only bucket assigned to the tree piece
      int bucketIdx = tp.bucketStartIdx;
      CkAssert(tp.bucketStartIdx+1 == tp.bucketEndIdx);
      //CkPrintf("%d REGISTER %llu bucket idx %d\n", CkMyPe(), node->getKey(), bucketIdx);
      myBuckets[bucketIdx] = node;
    }

    for(int j = tp.bucketStartIdx; j < tp.bucketEndIdx; j++){
      buckets << myBuckets[j]->getKey() << ","; 
    }

    tp.root = node;
    //CkPrintf("%d REGISTER tree piece %d root %llu buckets %s\n", CkMyPe(), tp.index, tp.root->getKey(), buckets.str().c_str());
  }
  else{
    Node<ForceData> *rightChild = node->getRightChild();
    Node<ForceData> *leftChild = node->getLeftChild();
    int rightFirstOwner = rightChild->getOwnerStart(); 
    int tp = binary_search_ge<int,TreePieceDescriptor>(rightFirstOwner,localTreePieces.submittedParticles.getVec(),tpstart,tpend);

    registerTopLevelNodes(leftChild,tpstart,tp);
    registerTopLevelNodes(rightChild,tp,tpend);
  }
}

void DataManager::flushBufferedRemoteDataRequests(){
  CkAssert(treeMomentsReady);
  for(int i = 0; i < bufferedNodeRequests.length(); i++){
    requestNode(bufferedNodeRequests[i]);
  }
  for(int i = 0; i < bufferedParticleRequests.length(); i++){
    requestParticles(bufferedParticleRequests[i]);
  }
  bufferedNodeRequests.length() = 0;
  bufferedParticleRequests.length() = 0;
}

bool CompareNodePtrToKey(void *a, Key k){
  Node<ForceData> *node = *((Node<ForceData>**)a);
  return (Node<ForceData>::getParticleLevelKey(node) >= k);
}

void DataManager::startTraversal(){
  LBTurnInstrumentOn();

  /*
  CkPrintf("(%d) memcheck before traversal\n", CkMyPe());
  */

  Node<ForceData> **bucketPtrs = myBuckets.getVec();
#if 0
  localTreePieces.submittedParticles[0].bucketStartIdx = 0;
#endif
  int start = 0;
  int end = myBuckets.length();

  //doPrintTree();
  //
  if(numLocalUsefulTreePieces > 0){
    int dummy=0;
    for(int i = 0; i < numLocalUsefulTreePieces; i++){
      TreePieceDescriptor &tp = localTreePieces.submittedParticles[i];
      ostringstream buckets;
      for(int j = tp.bucketStartIdx; j < tp.bucketEndIdx; j++){
        buckets << myBuckets[j]->getKey() << ","; 
      }
      //CkPrintf("DM %d tree piece %d root %llu buckets %s\n", CkMyPe(), tp.index, tp.root->getKey(), buckets.str().c_str());
      tp.owner->prepare(root, tp.root, myBuckets.getVec()+tp.bucketStartIdx, tp.bucketEndIdx-tp.bucketStartIdx);
      CkEntryOptions opts;
      opts.setPriority(START_TRAVERSAL_PRIORITY);
      opts.setQueueing(CK_QUEUEING_IFIFO);
      treePieceProxy[tp.index].startTraversal(dummy,&opts);
    }
  }
  else{
    finishIteration();
  }

#if 0
  for(int i = 0; i < myNumParticles; i++){
    Particle *p = &myParticles[i];
    Real particleKinetic = 0.5*p->mass*p->velocity.lengthSquared();
    Real particlePotential = p->mass*p->potential;
    Real particleEnergy = particleKinetic+particlePotential;
    CkPrintf("%d before traversal iteration %d energy K %f pos %f %f %f v %f %f %f\n", p->id, iteration, particleKinetic, p->position.x, p->position.y, p->position.z, p->velocity.x, p->velocity.y, p->velocity.z);
  }

#endif


#if 0
  if(end > 0){
    for(int i = 0; i < numLocalTreePieces-1; i++){
      TreePieceDescriptor &descr = submittedParticles[i];
      int bucketIdx = binary_search_ge<Node<ForceData>*>(descr.largestKey,bucketPtrs,start,end,CompareNodePtrToKey);
      descr.bucketEndIdx = bucketIdx;
      int tpIndex = descr.owner->getIndex();
      descr.owner->prepare(root,localTPRoots[tpIndex],myBuckets.getVec(),descr.bucketStartIdx,descr.bucketEndIdx);
      treePieceProxy[tpIndex].tartTraversal();
      submittedParticles[i+1].bucketStartIdx = bucketIdx;
      start = bucketIdx;
    }
    TreePieceDescriptor &descr = submittedParticles[numLocalTreePieces-1];
    descr.bucketEndIdx = myBuckets.length();
    int tpIndex = descr.owner->getIndex();
    descr.owner->prepare(root,localTPRoots[tpIndex],myBuckets.getVec(),descr.bucketStartIdx,descr.bucketEndIdx);
    treePieceProxy[tpIndex].tartTraversal();
  }
  else if(numLocalTreePieces > 0){
    for(int i = 0; i < numLocalTreePieces; i++){
      TreePieceDescriptor &descr = submittedParticles[i];
      descr.owner->prepare(root,NULL,myBuckets.getVec(),0,0);
      int tpIndex = descr.owner->getIndex();
      treePieceProxy[tpIndex].tartTraversal();
    }
  }
  else{
    finishIteration();
  }
#endif
}

ExternalParticle *DataManager::requestParticles(Node<ForceData> *leaf, CutoffWorker<ForceData> *worker, State *state, Traversal<ForceData> *traversal){
  Key key = leaf->getKey();
  Request &request = particleRequestTable[key];
#ifdef TRACE_REMOTE_DATA_REQUESTS
  traceUserEvent(REMOTE_PARTICLE_REQUEST);
#endif
  if(request.data != NULL){
    return (ExternalParticle *)request.data;
  }
  else if(!request.sent){
    partReqs.incrRequests();

    request.sent = true;
    request.parentCached = leaf->isCached();
    if(leaf->isCached()){
      request.parent = leaf;
    }
    else{
      request.parent = new Node<ForceData>(*leaf);
      CkAssert(!request.parent->isCached());
    }

    CkAssert(leaf->getOwnerStart()+1 == leaf->getOwnerEnd());
    int owner = leaf->getOwnerStart();
    CkEntryOptions opts;
    opts.setQueueing(CK_QUEUEING_IFIFO);
    opts.setPriority(REQUEST_PARTICLES_PRIORITY);
    treePieceProxy[owner].requestParticles( std::make_pair(key, CkMyPe()), &opts);
    
    RRDEBUG("(%d) REQUEST particles %lu leafCached %d from tp %d\n", CkMyPe(), key, leaf->isCached(), owner);
  }
  request.requestors.push_back(Requestor(worker,state,traversal,worker->getContext()));
  partReqs.incrDeliveries();
  return NULL;
}

void DataManager::requestParticles(std::pair<Key, int> &request) {
  if(!treeMomentsReady){
    bufferedParticleRequests.push_back(request);
    return;
  }

  RRDEBUG("(%d) REPLY particles key %lu to %d\n", CkMyPe(), request.first, request.second);

  map<Key,Node<ForceData>*>::iterator it = nodeTable.find(request.first);
  CkAssert(it != nodeTable.end());
  Node<ForceData> *bucket = it->second;
  CkAssert(bucket->getType() == Bucket);

  Particle *data = bucket->getParticles();
  int np = bucket->getNumParticles();

  ParticleReplyMsg *pmsg = new (np,NUM_PRIORITY_BITS) ParticleReplyMsg;
  *(int *)CkPriorityPtr(pmsg) = RECV_PARTICLES_PRIORITY;
  CkSetQueueing(pmsg,CK_QUEUEING_IFIFO);

  pmsg->key = request.first;
  pmsg->np = np;
  for(int i = 0; i < np; i++){
    pmsg->data[i] = data[i];
  }

  myProxy[request.second].recvParticles(pmsg);
}

Node<ForceData>* DataManager::requestNode(Node<ForceData> *leaf, CutoffWorker<ForceData> *worker, State *state, Traversal<ForceData> *traversal){
  Key key = leaf->getKey();
  Request &request = nodeRequestTable[key];
#ifdef TRACE_REMOTE_DATA_REQUESTS
  traceUserEvent(REMOTE_NODE_REQUEST);
#endif
  if(request.data != NULL){
    RRDEBUG("(%d) REQUEST node %lu have data!\n", CkMyPe(), key);
    return (Node<ForceData> *)request.data;
  }
  else if(!request.sent){
    nodeReqs.incrRequests();

    request.sent = true;
    request.parentCached = leaf->isCached();
    if(leaf->isCached()){
      request.parent = leaf;
    }
    else{
      // in order to keep cached tree separate from 
      // local tree, since we can share trees across 
      // processors on an SMP node
      // by default, request.parentCached should be false
      request.parent = new Node<ForceData>(*leaf);
    }

    int numOwners = leaf->getOwnerEnd()-leaf->getOwnerStart();
    int requestOwner = leaf->getOwnerStart()+(rand()%numOwners);
    RRDEBUG("(%d) REQUEST node %lu leafCached %d from tp %d\n", CkMyPe(), key, leaf->isCached(), requestOwner);
#ifdef COMBINE_NODE_REQUESTS
    combineNodeRequest(requestOwner,key);
#else
    CkEntryOptions opts;
    opts.setPriority(REQUEST_NODE_PRIORITY);
    opts.setQueueing(CK_QUEUEING_IFIFO);
    std::pair<Key,int> pr(key,CkMyPe());
    treePieceProxy[requestOwner].requestNode(pr,&opts);
    /*
    RequestMsg *msg = new (NUM_PRIORITY_BITS) RequestMsg(key,CkMyPe());
    CkSetQueueing(msg,CK_QUEUEING_IFIFO);
    *((int *)CkPriorityPtr(msg)) = REQUEST_NODE_PRIORITY; 
    treePieceProxy[requestOwner].requestNode(msg);
    */
#endif
  }
  request.requestors.push_back(Requestor(worker,state,traversal,worker->getContext()));
  nodeReqs.incrDeliveries();
  return NULL;
}

void DataManager::combineNodeRequest(int tpindex, Key k){
  int dest_pe = tpArray->lastKnown(CkArrayIndex1D(tpindex)); 
  //CkPrintf("[COMBINE] send tp %d key %llu reply %d dest_pe %d\n", tpindex, k, CkMyPe(), dest_pe);
  NodeRequest req(tpindex,k,CkMyPe());
  combiner->insertData(req, dest_pe);
}

void DataManager::process(NodeRequest &req){
  int dest_pe = tpArray->lastKnown(CkArrayIndex1D(req.tp));
  // The target tree piece is on this PE: we must have its nodes
  if(dest_pe == CkMyPe()){
    //CkPrintf("[COMBINE] recv tp %d key %llu reply %d dest_pe %d\n", req.tp, req.key, req.replyTo, dest_pe);
    std::pair<Key,int> pr(req.key, req.replyTo);
    requestNode(pr);
  }
  else{
    // The tree piece that this request was intended for has migrated,
    // forward request to dest_pe
    //CkPrintf("[COMBINE] forward tp %d key %llu reply %d dest_pe %d\n", req.tp, req.key, req.replyTo, dest_pe);
    combiner->insertData(req, dest_pe);
  }
#if 0
  else{
    RequestMsg *msg = new RequestMsg(req.key, req.replyTo);
    tpArray->deliver((CkArrayMessage*)msg, CkDeliver_queue);
  }
#endif
}

void DataManager::doneRemoteRequests(){
  numTreePiecesDoneRemoteRequests++;
  if(numTreePiecesDoneRemoteRequests == numLocalUsefulTreePieces){
    //CkPrintf("[COMBINE] Turn off streaming on PE %d\n", CkMyPe());
    combiner->doneInserting();
  }
}

void DataManager::requestNode(std::pair<Key,int> &request){
  if(!treeMomentsReady){
    bufferedNodeRequests.push_back(request);
    return;
  }

  processNodeRequest(request.first,request.second);
}

void DataManager::processNodeRequest(Key key, int replyTo){
  CkAssert(treeMomentsReady);
  RRDEBUG("(%d) REPLY node %lu to %d\n", CkMyPe(), key, replyTo);

  map<Key,Node<ForceData>*>::iterator it = nodeTable.find(key);
  CkAssert(it != nodeTable.end());
  Node<ForceData> *node = it->second;

  CkAssert(node->getNumChildren() > 0);

  TreeSizeWorker tsz(node->getDepth()+globalParams.cacheLineSize);
  fillTrav.topDownTraversal_local(node,&tsz);

  int nn = tsz.getNumNodes();

  NodeReplyMsg *nmsg = new (nn,NUM_PRIORITY_BITS) NodeReplyMsg;
  *(int *)CkPriorityPtr(nmsg) = RECV_NODE_PRIORITY;
  CkSetQueueing(nmsg,CK_QUEUEING_IFIFO);

  nmsg->key = key;
  nmsg->nn = nn;

  Node<ForceData> *emptyBuf = nmsg->data;
  node->serialize(NULL,emptyBuf,globalParams.cacheLineSize);
  CkAssert(emptyBuf == nmsg->data+nn);

  myProxy[replyTo].recvNode(nmsg);
}

void DataManager::recvParticles(ParticleReplyMsg *msg){
  map<Key,Request>::iterator it = particleRequestTable.find(msg->key);
  CkAssert(it != particleRequestTable.end());

  RRDEBUG("(%d) RECVD particle REPLY for key %lu\n", CkMyPe(), msg->key);

  Request &req = it->second;
  CkAssert(req.requestors.length() > 0);
  CkAssert(req.sent);
  CkAssert(req.msg == NULL);

  req.msg = msg;
  req.data = msg->data;
  
  // attach particles to bucket in tree 
  Node<ForceData> *leaf = req.parent;
  CkAssert(leaf != NULL);
  CkAssert(leaf->getType() == RemoteBucket);
  leaf->setParticles((Particle *)msg->data,msg->np);

  partReqs.decrRequests();
  partReqs.decrDeliveries(req.requestors.length());
  req.deliverParticles(msg->np);
}

void DataManager::recvNode(NodeReplyMsg *msg){
  map<Key,Request>::iterator it = nodeRequestTable.find(msg->key);
  CkAssert(it != nodeRequestTable.end());


  Request &req = it->second;
  CkAssert(req.requestors.length() > 0);
  CkAssert(req.sent);
  CkAssert(req.msg == NULL);

  req.msg = msg;
  req.data = msg->data;
  
  // attach recvd subtree to appropriate point in local tree

  Node<ForceData> *node = req.parent;
  CkAssert(node != NULL);

  RRDEBUG("(%d) RECVD node REPLY for key %llu parent %llu cached %d\n", CkMyPe(), msg->key, node->getKey(), node->isCached());
  
  node->deserialize(msg->data, msg->nn);

  nodeReqs.decrRequests();
  nodeReqs.decrDeliveries(req.requestors.length());

  req.deliverNode();
}

void DataManager::traversalsDone(CmiUInt8 pnInter, CmiUInt8 ppInter, CmiUInt8 openCrit)
{
  numTreePiecesDoneTraversals++;
  //CkPrintf("DM %d traversalsDone %d\n", CkMyPe(), numTreePiecesDoneTraversals);
  numInteractions[0] += pnInter;
  numInteractions[1] += ppInter;
  numInteractions[2] += openCrit;
  if(numTreePiecesDoneTraversals == numLocalUsefulTreePieces){
    finishIteration();
  }
}

void DataManager::finishIteration(){

  if(CkMyPe()%numPesPerNode == 0){
    string name("merged");
    //doPrintTree(name);
  }


  //CkPrintf("DM %d finishIteration\n", CkMyPe());
  // can't advance particles here, because other PEs 
  // might not have finished their traversals yet, 
  // and therefore might need my particles

  CkAssert(nodeReqs.test());
  CkAssert(partReqs.test());

  DtReductionStruct dtred;
#if 0
  findMinVByA(dtred);
#endif

  dtred.pnInteractions = numInteractions[0];
  dtred.ppInteractions = numInteractions[1];
  dtred.openCrit = numInteractions[2];

  CkCallback cb(CkIndex_DataManager::advance(NULL),myProxy);
  contribute(sizeof(DtReductionStruct),&dtred,DtReductionType,cb);

}

void DataManager::advance(CkReductionMsg *msg){

  DtReductionStruct *dtred = (DtReductionStruct *)(msg->getData());
#if 0
  if(dtred->haveNaN){
    CkPrintf("(%d) iteration %d NaN accel detected! Exit...\n", CkMyPe(), iteration);
    markNaNBuckets();
    CkCallback exitCb(CkCallback::ckExit);
    contribute(0,0,CkReduction::sum_int,exitCb);
    return;
  }
#endif

  myBox.reset();
  kickDriftKick(myBox.box,myBox.energy);

  Real pad = 0.00001;
  myBox.expand(pad);
  myBox.numParticles = myNumParticles;

  if(CkMyPe() == 0){
    CkPrintf("[STATS] node inter %llu part inter %llu open crit %llu\n", dtred->pnInteractions, dtred->ppInteractions, dtred->openCrit);
  }

  iteration++;
  
  if(iteration == 13){
    traceBegin();
  }
  else if(iteration == 17){
    traceEnd();
  }

  if(iteration % globalParams.decompPeriod != 0){ 
    doSkipDecomposition = true;
  }
  else{
    doSkipDecomposition = false;
  }


  if(iteration == globalParams.iterations){
    CkCallback cb = CkCallback(CkIndex_Main::niceExit(),mainProxy);
    contribute(0,0,CkReduction::sum_int,cb);
  }
  else if(iteration % globalParams.balancePeriod == 0){
    if(CkMyPe() == 0) CkPrintf("(%d) INITIATE LB\n", CkMyPe()); 
    for(int i = 0; i < localTreePieces.submittedParticles.length(); i++){
      TreePiece *tp = localTreePieces.submittedParticles[i].owner;
      tp->startlb();
    }
    // must do decomposition after a load balancing step
    doSkipDecomposition = false;
  }
  else{
    init();
    for(int i = 0; i < localTreePieces.submittedParticles.length(); i++){
      TreePiece *tp = localTreePieces.submittedParticles[i].owner;
      tp->cleanup();
    }
    CkCallback cb = CkCallback(CkIndex_DataManager::recvUnivBoundingBox(NULL),myProxy);
    contribute(sizeof(BoundingBox),&myBox,BoundingBoxGrowReductionType,cb);
  }

  delete msg;
}

void DataManager::recvUnivBoundingBox(CkReductionMsg *msg){
  BoundingBox &univBB = *((BoundingBox *)msg->getData());
  decompose(univBB);
  delete msg;
}

extern string NodeTypeString[];

void DataManager::freeCachedData(){
  map<Key,Request>::iterator it;


  for(it = particleRequestTable.begin(); it != particleRequestTable.end(); it++){
    Request &request = it->second;
    CkAssert(request.sent);
    CkAssert(request.data != NULL);
    CkAssert(request.requestors.length() == 0);
    CkAssert(request.msg != NULL);

    CkAssert(request.parentCached == request.parent->isCached());
    
    if(!request.parentCached){
      //CkPrintf("[%d] delete uncached bucket %llu type %s cached %d\n", CkMyPe(), request.parent->getKey(), NodeTypeString[request.parent->getType()].c_str(), request.parent->isCached());
      delete request.parent;
    }

    delete (ParticleReplyMsg *)(request.msg);
  }

  for(it = nodeRequestTable.begin(); it != nodeRequestTable.end(); it++){
    Request &request = it->second;
    CkAssert(request.sent);
    CkAssert(request.data != NULL);
    CkAssert(request.requestors.length() == 0);
    CkAssert(request.msg != NULL);

    CkAssert(request.parentCached == request.parent->isCached());

    if(!request.parentCached){
      //CkPrintf("[%d] delete uncached node %llu type %d cached %d %d\n", CkMyPe(), request.parent->getKey(), request.parent->getType(), request.parentCached, request.parent->isCached());
      delete request.parent;
    }

    delete (NodeReplyMsg *)(request.msg);
  }

  nodeRequestTable.clear();
  particleRequestTable.clear();
}

void DataManager::quiescence(){
  CkPrintf("QUIESCENCE dm %d pieces done %d (%d) nodereq %d partreq %d\n",
              CkMyPe(),
              numTreePiecesDoneTraversals,
              localTreePieces.count,
              nodeReqs.test(),
              partReqs.test()
              );
  
  CkCallback cb(CkIndex_Main::quiescenceExit(),mainProxy);
  contribute(0,0,CkReduction::sum_int,cb);
}

void DataManager::freeTree(){
#ifdef NODE_LEVEL_MERGE
  // delete the trees underneath your tree pieces
  // this can be done in parallel by all PEs on node
  Node<ForceData> *treePieceRoot;
  for(int i = 0; i < numLocalUsefulTreePieces; i++){
    treePieceRoot = localTreePieces.submittedParticles[i].root;
    CkAssert(treePieceRoot != NULL);
    treePieceRoot->deleteBeneath();
  }
#endif


  if(root != NULL){
#ifdef NODE_LEVEL_MERGE
    //CkPrintf("DM %d freeMergedTree\n", CkMyPe());
    treeMergerProxy.ckLocalBranch()->freeMergedTree();
#else
    root->deleteBeneath();
    delete root;
    root = NULL;
#endif
  }

}

void DataManager::reuseTree(){
#ifdef NODE_LEVEL_MERGE
  //CkAbort("Don't reuse decomposition tree: needs to be fixed for NODE_LEVEL_MERGE");
  Node<ForceData> *tproot;
  for(int i = 0; i < numLocalUsefulTreePieces; i++){
    tproot = localTreePieces.submittedParticles[i].root;
    CkAssert(tproot != NULL);
    tproot->reuseTree();
  }
#endif

  if(root != NULL){
#ifdef NODE_LEVEL_MERGE
    treeMergerProxy.ckLocalBranch()->reuseMergedTree();
#else
    root->reuseTree();
#endif
  }
}

void DataManager::kickDriftKick(OrientedBox<double> &box, Real &energy){
  Vector3D<Real> dv;

  Particle *pstart = myParticles.getVec();
  Particle *pend = pstart+(myNumParticles-1);

  Real particleEnergy;
  Real particleKinetic;
  Real particlePotential;
  

  if(globalParams.doPrintAccel && (iteration == globalParams.iterations-1)){
    Node<ForceData> *bucket;
    for(int i = 0; i < myBuckets.length(); i++){
      bucket = myBuckets[i];
      ostringstream oss;
      oss << "final bucket " << bucket->getKey() << " PE " << CkMyPe() << ": ";
      for(Particle *p = bucket->getParticles(); p != bucket->getParticles()+bucket->getNumParticles(); p++){
        oss << p->acceleration.x << " " << p->acceleration.y << " " << p->acceleration.z << " ;";
      }
      CkPrintf("%s\n", oss.str().c_str());
    }
  }

  for(Particle *p = pstart; p <= pend; p++){
    particlePotential = p->mass*p->potential;
    particleKinetic = 0.5*p->mass*p->velocity.lengthSquared();
    particleEnergy = particlePotential+particleKinetic;
    energy += particleEnergy;


    // kick
    p->velocity += globalParams.dthf*p->acceleration;
#ifndef NO_DRIFT
    // drift
    p->position += globalParams.dtime*p->velocity;
#endif
    // kick
    p->velocity += globalParams.dthf*p->acceleration;
    
    box.grow(p->position);

    p->acceleration = Vector3D<Real>(0.0);
    p->potential = 0.0;

#if 0
    particlePotential = p->mass*p->potential;
    particleKinetic = 0.5*p->mass*p->velocity.lengthSquared();
    particleEnergy = particlePotential+particleKinetic;
    CkPrintf("%d after update iteration %d energy K %f pos %f %f %f v %f %f %f\n", p->id, iteration, particleKinetic, p->position.x, p->position.y, p->position.z, p->velocity.x, p->velocity.y, p->velocity.z);
#endif
  }
}

#if 0
void DataManager::findMinVByA(DtReductionStruct &dtred){
  if(myNumParticles == 0) {
    dtred.haveNaN = false;
    return;
  }
  
  dtred.haveNaN = false;

  for(int i = 0; i < myNumParticles; i++){
    Real v = myParticles[i].velocity.length();
    Real a = myParticles[i].acceleration.length();
    CkAssert(!isnan(v));
    if(isnan(a)) dtred.haveNaN = true;
  }

}
#endif

#if 0
void DataManager::markNaNBuckets(){
  for(int i = 0; i < myBuckets.length(); i++){
    Node<ForceData> *bucket = myBuckets[i];
    Particle *part = bucket->getParticles();
    int numParticles = bucket->getNumParticles();
    for(int j = 0; j < numParticles; j++){
      if(isnan(part[j].acceleration.length())){
        bucket->setType(Invalid);
        break;
      }
    }
  }
}
#endif

void DataManager::init(){
  LBTurnInstrumentOff();
  CkAssert(pendingMoments.empty());
  // safe to reset here, since all tree pieces 
  // must have finished iteration
  freeCachedData();

  decompIterations = 0;

#if 0
  doneFlushParticles = false;
#endif
  doneTreeBuild = false;
#if 0
  numLocalTreePieces = -1;
#endif
  myBuckets.length() = 0;
  treeMomentsReady = false;
  numTreePiecesDoneTraversals = 0;
  numTreePiecesDoneRemoteRequests = 0;

  if(doSkipDecomposition){
    reuseTree();
  }
  else{
    freeTree();
    nodeTable.clear();
  }

  CkAssert(activeBins.getNumCounts() == 0);

  numInteractions[0] = 0;
  numInteractions[1] = 0;
  numInteractions[2] = 0;
#if 0
  submittedParticles.length() = 0;
#endif

  numMomentsRequested = numMomentsReceived = 0;
}

void DataManager::resumeFromLB(){
  /* we delay the freeing of data structures to this point
   * because we want the tree pieces to be able to use the
   * tree, and in particular their roots for load balancing
   * */
  init();
  CkCallback cb(CkIndex_DataManager::recvUnivBoundingBox(NULL),myProxy);
  contribute(sizeof(BoundingBox),&myBox,BoundingBoxGrowReductionType,cb);
}

extern string NodeTypeColor[];
void DataManager::printTree(Node<ForceData> *nd, ostream &os){
  os << nd->getKey() 
     << "[label=\""<< nd->getKey() 
     << "," << nd->getNumParticles() 
     << "," << nd->getOwnerStart() << ":" << nd->getOwnerEnd() 
     << "\\n" << nd->data.moments.cm
     << "\","
     << "style=\"filled\""
     << "color=\"" << NodeTypeColor[nd->getType()] << "\""
     << "]" << endl;
  //if(nd->getOwnerEnd()-1 == nd->getOwnerStart()) return;
  for(int i = 0; i < nd->getNumChildren(); i++){
    Node<ForceData> *child = nd->getChildren()+i;
    if(child != NULL){
      os << nd->getKey() << " -> " << child->getKey() << endl;
      printTree(child,os);
    }
  }
}
#if 0
#endif

void DataManager::doPrintTree(string name){
  ostringstream oss;
  oss << name << "." << CkMyPe() << "." << iteration << ".dot";
  ofstream ofs(oss.str().c_str());
  ofs << "digraph " << name << "_" << CkMyPe() << "_" << iteration << " {" << endl;
  if(root != NULL) printTree(root,ofs);
  ofs << "}" << endl;
  ofs.close();
}

int DataManager::flushAndMark(Node<ForceData> *node, int leafNum){
  node->setOwnerStart(leafNum);
  int firstNotInLeft, firstNotInNode;
  if(node->getNumChildren() > 0){
    firstNotInLeft = flushAndMark(node->getLeftChild(),leafNum);
    firstNotInNode = flushAndMark(node->getRightChild(),firstNotInLeft);
  }
  else{
    // sendParticlesToTreePiece whose root is 'node'
    sendParticlesToTreePiece(node,leafNum);
    firstNotInNode = leafNum+1;
  }
  node->setOwnerEnd(firstNotInNode);
  return firstNotInNode;
}

// Each node should have been marked with the 
// min and max tree piece indices that it
// hosts beneath it. Use this information to
// obtain the roots of the 'localTreePieces' 
// and build trees there. 
// This procedure only goes
// down to the depths of TreePiece roots; after that,
// the singleBuildTree function is invoked.
void DataManager::buildTree(Node<ForceData> *node, int pstart, int pend, int tpstart, int tpend){
  TB_DEBUG("(%d) pstart %d pend %d tpstart %d tpend %d node %lu\n", CkMyPe(), pstart, pend, tpstart, tpend, node->getKey());
  
  int np;
  nodeTable[node->getKey()] = node;

  if(tpend <= tpstart){
    // No local tree piece under this node
    // It is Remote/RemoteBucket/RemoteEmptyBucket
    // Make request for remote node
    int numOwners = node->getOwnerEnd()-node->getOwnerStart();
    int requestOwner = node->getOwnerStart()+(rand()%numOwners);
    TB_DEBUG("(%d) requestMoments from tree piece %d for node %lu\n", CkMyPe(), requestOwner, node->getKey());

    CkEntryOptions opts;
    opts.setQueueing(CK_QUEUEING_IFIFO);
    opts.setPriority(REQUEST_MOMENTS_PRIORITY);
    treePieceProxy[requestOwner].requestMoments(node->getKey(),CkMyPe(),&opts);
    numMomentsRequested++;

    // There are no particles on this PE under this node
    CkAssert(pstart == pend);
    node->setParticles(NULL,0);
    // Delete subtree beneath this node
    //if(node->getNumChildren() > 0) CkPrintf("(%d) REMOTE deleteBeneath %llu\n", CkMyPe(), node->getKey());
    node->deleteBeneath();
    // Set type when requested moments are received
    // Don't tell parent that I'm done

    return;
  }
  else if(tpend-tpstart == 1 && (node->getOwnerEnd()-node->getOwnerStart()==1)){
    TreePieceDescriptor &currentTP = localTreePieces.submittedParticles[tpstart];
    np = currentTP.numParticles;
    TB_DEBUG("(%d) SINGLE LOCAL tree piece %d for node %lu\n", CkMyPe(), currentTP.index, node->getKey());
    //CkPrintf("(%d) SINGLE TREE index %d np %d pstart %d pend %d\n", CkMyPe(), currentTP.index, np, pstart, pend);
    // This is the first node that has 
    // a single tree piece beneath it.

    // Construct the entire tree underneath this node
    // and report the first particle index that doesn't
    // lie within it
    CkAssert(node->getOwnerStart() == currentTP.index);
    // There must be these many particles under the root of this TreePiece
    CkAssert(np == pend-pstart);

    if(np > 0) node->setParticles(myParticles.getVec()+pstart,np);
    else node->setParticles(NULL,0);

    // Set the bucket indices for this TreePiece
    // If this is not the 0-th local TreePiece to have its bucket indices set,
    // it can use the last bucket (exclusive) of the one previous to it as its first bucket
    if(tpstart > 0) currentTP.bucketStartIdx = localTreePieces.submittedParticles[tpstart-1].bucketEndIdx; 
    // To obtain the last bucket, set it to the first
    // bucket and increment each time a new bucket is encountered (in singleBuildTree)
    currentTP.bucketEndIdx = currentTP.bucketStartIdx;
    // Build the completely local tree underneath the root current TreePiece
    singleBuildTree(node,currentTP);
    // Since this node was completely local, its moments must have been computed
    node->setChildrenMomentsReady();
    // Tell parent node that one of its children has computed its moments
    notifyParentMomentsDone(node);

    // Set the root of the TreePiece
    currentTP.root = node;
    return;
  }


  np = pend-pstart;
  Node<ForceData> *leftChild = node->getLeftChild();
  Node<ForceData> *rightChild = node->getRightChild();

  if(np > 0) node->setParticles(myParticles.getVec()+pstart,pend-pstart);
  else node->setParticles(NULL,0);

  // If we are here, this node has at least one local tree piece underneath it
  CkAssert(tpend > tpstart);

  // Make sure that the range of tree pieces
  // is contained within this node; otherwise,
  // we shouldn't have made this call at all.
  CkAssert(localTreePieces.submittedParticles[tpstart].index >= node->getOwnerStart());

  // Find the first local TreePiece that has
  // an index beyond the left child's last
  // contained TreePiece, i.e. equal to or after
  // the right child's first contained TreePiece 
  int rightFirstOwner = rightChild->getOwnerStart(); 
  int tp = binary_search_ge<int,TreePieceDescriptor>(rightFirstOwner, localTreePieces.submittedParticles.getVec(), tpstart, tpend); 

  Key particleTestKey = Node<ForceData>::getParticleLevelKey(rightChild);
  int firstParticleNotInLeft = binary_search_ge<Key,Particle>(particleTestKey, myParticles.getVec(), pstart, pend);

  buildTree(leftChild,pstart,firstParticleNotInLeft,tpstart,tp);
  buildTree(rightChild,firstParticleNotInLeft,pend,tp,tpend);

  if(node->allChildrenMomentsReady()){
    // All descendants were able to construct their subtrees
    // from data local to this PE: they must all be internal,
    // and so must this node. Note that this node cannot be
    // a Bucket or EmptyBucket, since it has more than one
    // local TreePiece underneath it
    node->setType(Internal);
    // Create node's moments from those of children
    node->getMomentsFromChildren();
    // Tell node's parent it is done building subtree
    notifyParentMomentsDone(node);
  }
  else{
    // Some descendants of node were unable to
    // construct their moments from PE-local data,
    // i.e. they are Remote: therefore, this node must be 
    // Boundary. It CANNOT be Remote since tpstart < tpend for it
    node->setType(Boundary);
  }
}

void DataManager::notifyParentMomentsDone(Node<ForceData> *node){
  // Respond to any requestors for the moments of 'node'
  TB_DEBUG("(%d) Notify parent flushing moments for node %lu\n", CkMyPe(), node->getKey());
  momentsResponseHelper(node);

  Node<ForceData> *parent = node->getParent();
  if(parent != NULL) parent->childMomentsReady();
  else{
    // The root's moments have been computed,
    // i.e. all particles were internal to this PE.
    CkAssert(node->getKey() == Key(1));
    treeReady();
  }
}

// Recursive method to build the tree under a single TreePiece.
// given an array of particles
void DataManager::singleBuildTree(Node<ForceData> *node, TreePieceDescriptor &tp){
  node->setOwners(tp.index,tp.index+1);
  //nodeTable[node->getKey()] = node;

  int np = node->getNumParticles();
  if(np <= ((Real)globalParams.ppb*BUCKET_TOLERANCE)){
    if(np == 0) node->setType(EmptyBucket);
    else node->setType(Bucket);
    node->getMomentsFromParticles();
    // Add to the list of buckets on this PE
    myBuckets.push_back(node);
    // Record the fact that current tree piece has another bucket
    tp.bucketEndIdx++;

    // since we might be reusing the tree, it could happen
    // that this node was internal (not a leaf) in the previous 
    // iteration. delete descendants, if there are any
    //if(node->getNumChildren() > 0) CkPrintf("(%d) BUCKET deleteBeneath %llu\n", CkMyPe(), node->getKey());
    node->deleteBeneath();
  }
  else{
    node->setType(Internal);
    // Shouldn't not have allocated any children yet.
    // !doSkipDecomposition => node->getNumChildren() == 0
    CkAssert(doSkipDecomposition || (node->getNumChildren() == 0));
    // Partition the particles under this
    // node among the two children. To do this,
    // find the first particle that does not belong
    // underneath the left child, which is the same
    // as the first child that does belong under the
    // right child.
    if(node->getNumChildren() == 0) node->refine();
    else node->reuseRefine();

    Node<ForceData> *left = node->getLeftChild();
    Node<ForceData> *right = node->getRightChild();
    singleBuildTree(left,tp);
    singleBuildTree(right,tp);
    nodeTable[left->getKey()] = left;
    nodeTable[right->getKey()] = right;
    node->getMomentsFromChildren();
  }
}

void TreePieceCounter::addLocation(CkLocation &loc) {
  const int *indexData = loc.getIndex().data();
  TreePiece *tp = treePieceProxy[indexData[0]].ckLocal();
  int np = tp->getNumParticles();
  submittedParticles.push_back(TreePieceDescriptor(tp->getBufferedParticleMsgs(), np, tp, indexData[0]));
  numParticles += np;
  count++;
}

void DataManager::pup(PUP::er &p){
  p|numRankBits;
  p|prevIterationStart;

  p|myParticles;
  p|myNumParticles;
  p|myBox;

  p|numTreePieces;
  p|iteration;

  p|compareEnergy;

  p|myProxy;
  if(p.isUnpacking()){
    combiner = ((MeshStreamer<NodeRequest> *)CkLocalBranch(combinerProxy));
    tpArray = treePieceProxy.ckGetArrayID().ckLocalBranch();
  }


}


#include "Traversal_defs.h"

