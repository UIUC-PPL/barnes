/* 
  CharmBH - DataManager group (one member per PE)
  Each PE does the following:

  1. Loads its shares of particles from input file.
  2. Decomposes the particles onto tree pieces.
  3. After decomposition, accepts particles from all
     tree pieces hosted on PE to build combined tree
     from them.
  4. Initiates traversals for its tree pieces after 
     on-PE tree is built.
  5. Maintains a cache of remote data requested by
     its tree pieces.
  6. Serves local data requests generated by remote PEs.
  7. Advances particles when traversals have finished
     on all PEs.

*/

#include "DataManager.h"
#include "Reduction.h"
#include "defines.h"
#include "Messages.h"
#include "Parameters.h"

#include "Worker.h"
#include "TreePiece.h"

#include "Request.h"
#include "defaults.h"

#include <fstream>
#include <iostream>
#include <sstream>

using namespace std;
extern CProxy_TreePiece treePieceProxy;
extern CProxy_Main mainProxy;
extern Parameters globalParams;

/*
  This function is called during tree building. When a 
  PE requests the data for a remote node, it receives only
  the type, moments and bounding box for it (since these
  are properties that cannot be calculated by this PE, given
  that it has no particles under the requested remote node).
*/
void copyMomentsToNode(Node<ForceData> *node, const MomentsExchangeStruct &mes){
  CkAssert(node->getKey() == mes.key);

  node->data.moments = mes.moments;
  node->data.box = mes.box;
  NodeType type = mes.type;
  node->setType(Node<ForceData>::makeRemote(type));

}

DataManager::DataManager() : 
  iteration(0),
  prevIterationStart(0.0),
  root(NULL),
  keyRanges(NULL), 
  rangeMsg(NULL)
{
  init();
}

/*
  Load your share of the particles from the input
  file. 
*/
void DataManager::loadParticles(CkCallback &cb){
  numRankBits = LOG_BRANCH_FACTOR;

  const char *fname = globalParams.filename;
  int npart = globalParams.numParticles;

  std::ifstream partFile;
  partFile.open(fname, ios::in | ios::binary);
  CkAssert(partFile.is_open());

  /*
    Calculate your share of input particles,
    and the offset into the input file from
    where you are to begin reading.
  */

  int offset = 0; 
  int myid = CkMyPe();
  int npes = CkNumPes();

  /*
    Calculate your share of input particles,
    and the offset into the input file from
    where you are to begin reading.
  */
  int avgParticlesPerPE = npart/npes;
  int rem = npart-npes*avgParticlesPerPE;
  if(myid < rem){
    avgParticlesPerPE++;
    offset = myid*avgParticlesPerPE;
  }
  else{
    offset = myid*avgParticlesPerPE+rem;
  }
  myNumParticles = avgParticlesPerPE;
  offset *= SIZE_PER_PARTICLE;
  offset += PREAMBLE_SIZE;

  /*
    Reserve enough space for your particles.
  */
  myParticles.reserve(myNumParticles);
  myParticles.length() = myNumParticles;

  /*
    Seek to correct starting position in input file.
  */
  partFile.clear();
  partFile.seekg(offset,ios::beg);
  if(partFile.fail()){
    std::ostringstream oss;
    oss << "couldn't seek to position " << offset << " on PE " << CkMyPe() << " position " << partFile.tellg() << endl;
    CkAbort(oss.str().c_str());
  }
  unsigned int numParticlesDone = 0;

  BoundingBox myBox;

  /*
    Read particles.
  */
  Real tmp[REALS_PER_PARTICLE];
  myBox.energy = 0.0;
  while(numParticlesDone < myNumParticles && !partFile.eof()){
    partFile.read((char *)tmp, SIZE_PER_PARTICLE);
    Particle &p = myParticles[numParticlesDone];
    p.position.x = tmp[0];
    p.position.y = tmp[1];
    p.position.z = tmp[2];
    p.velocity.x = tmp[3];
    p.velocity.y = tmp[4];
    p.velocity.z = tmp[5];
    p.mass = tmp[6];

    // these will be calculated during the course of the iteration
    p.acceleration = Vector3D<Real>(0.0);
    p.potential = 0.0;

    // in order to find the bounding box of your particles
    myBox.grow(p.position);
    // accumulate KE for this time period
    myBox.energy += p.mass*p.velocity.lengthSquared();

    numParticlesDone++;
  }
  myBox.energy /= 2.0;

  CkAssert(numParticlesDone == myNumParticles);
  myBox.numParticles = myNumParticles;

  partFile.close();

  /*
    Each PE contributes the bounding box of its particles to a reduction.
    At the root of the reduction (in Main.cc) we obtain the bounding box
    of all loaded particles. This bounding box is required in order to
    obtain the key for each particle.
  */
  contribute(sizeof(BoundingBox),&myBox,BoundingBoxGrowReductionType,cb);
}

/*
  Obtain a 64-bit key for each particle. This gives us a cheap way to
  arrange particles within the Barnes-Hut tree. The hash below is a 
  simple interleaving of the bits of each component of a particle's
  position vector.

  The idea is to locate each particle within a three-dimensional grid which
  has BOXES_PER_DIM points in each dimension, and whose extents are
  the same as those of the simulation universe. Thus, for each particle, 
  we use its position coordinates to obtain three integer values telling which
  of those grid points it falls on. The width of each integer is BOXES_PER_DIM,
  and by interleaving the bits of the three integers, we can tell the position
  of the particle in the Barnes-Hut tree.
*/
void DataManager::hashParticleCoordinates(OrientedBox<Real> &universe){
  Key prepend;
  prepend = 1L;
  prepend <<= (TREE_KEY_BITS-1);

  Real xsz = universe.greater_corner.x-universe.lesser_corner.x;
  Real ysz = universe.greater_corner.y-universe.lesser_corner.y;
  Real zsz = universe.greater_corner.z-universe.lesser_corner.z;

  for(unsigned int i = 0; i < myNumParticles; i++){
    Particle *p = &(myParticles[i]);
    // Obtain the integer grid points on which the particle falls in each dimension
    Key xint = ((Key) (((p->position.x-universe.lesser_corner.x)*(BOXES_PER_DIM*1.0))/xsz)); 
    Key yint = ((Key) (((p->position.y-universe.lesser_corner.y)*(BOXES_PER_DIM*1.0))/ysz)); 
    Key zint = ((Key) (((p->position.z-universe.lesser_corner.z)*(BOXES_PER_DIM*1.0))/zsz)); 

    // Interleave bits
    Key mask = Key(0x1);
    Key k = Key(0x0);
    int shiftBy = 0;
    for(int j = 0; j < BITS_PER_DIM; j++){
      k |= ((zint & mask) <<  shiftBy);
      k |= ((yint & mask) << (shiftBy+1));
      k |= ((xint & mask) << (shiftBy+2));
      mask <<= 1;
      // minus 1 because mask itself has shifted
      // left by one position
      shiftBy += (NDIMS-1);
    }
    // Prepend the key with a '1' bit.
    k |= prepend; 
    myParticles[i].key = k;
  }
}

/*
  Use the keys of the particles (as calculated above) 
  to decompose them onto tree pieces. Each tree piece
  gets a subvolume of the entire universe. This subvolume
  is represented in the tree by a particular node (internal
  or leaf). All particles within this subvolume fall beneath
  the node, so that the node's key is the common prefix of 
  all particles enclosed by the subvolume.

  Through the decomposition, we obtain several subsets of 
  particles, each of which constitutes a subvolume of the 
  universe. This subvolume corresponds to a tree piece, and
  the number of particles within it is guaranteed to be less
  than a user-specified threshold (ppc/particles per chare).

  The decomposition procedure is iterative, similar to
  histogram sort. In each iteration, a master PE (0) maintains
  a list of "active" nodes (leaves) which are to be partitioned.
  The keys of these nodes are broadcast to the workers (PE 0 is 
  both master and worker). The workers split the corresponding
  nodes into children and partition the particles previously in
  the active nodes to the appropriate children. They then contribute
  the number of particles they hold under each child to a reduction.
  The master uses the result of this reduction to find which nodes
  are to be split further: These are made active leaves for the 
  next iteration; this continues until there are no remaining active
  leaves.

  Decomposition begins at the root of the global tree, which 
  represents the bounding box enclosing all particles in the 
  simulated universe.
*/
void DataManager::decompose(BoundingBox &universe){

  // Obtain key for each particle given the extents of the universe.
  hashParticleCoordinates(universe.box);
  // Sort particles so that decomposition can be done in-place.
  myParticles.quickSort();

  if(CkMyPe()==0){
    // Check whether total energy remains (about) constant
    if(iteration == 1){
      // save this value so that we can compare
      // against it in future iterations. 
      compareEnergy = universe.energy;
    }
    else if(iteration > 1){
      Real deltaE = compareEnergy-universe.energy;
      if(deltaE < 0) deltaE = -deltaE;
      // The energy should grow in magnitude
      // by less than a tenth of one per cent.
      CkAssert(deltaE/compareEnergy < 0.001);
      CkPrintf("(%d) iteration %d deltaE/E %f\n", CkMyPe(), iteration, deltaE/compareEnergy);
    }

    // Print statistics
    float memMB = (1.0*CmiMemoryUsage())/(1<<20);
    ostringstream oss; 
    CkPrintf("(%d) prev time %g s\n", CkMyPe(), CmiWallTimer()-prevIterationStart);
    CkPrintf("(%d) mem %.2f MB\n", CkMyPe(), memMB);
    CkPrintf("(%d) iteration %d univ %f %f %f %f %f %f energy %f\n", 
              CkMyPe(),
              iteration,
              universe.box.lesser_corner.x,
              universe.box.lesser_corner.y,
              universe.box.lesser_corner.z,
              universe.box.greater_corner.x,
              universe.box.greater_corner.y,
              universe.box.greater_corner.z,
              universe.energy);
 

    prevIterationStart = CkWallTimer();
  }

  /* 
    We increase the number of required tree pieces
    as the decomposition iterations proceed. Begin
    with a single tree piece holding all the particles.
  */
  numTreePieces = 1;
  // How many particles do I hold?
  initHistogramParticles();
  // Send this count to the master PE
  sendHistogram();
}

/*
  Find out how many particles held by this PE
  fall under the root node (i.e. contribute the
  number of particles held by this PE)
*/
void DataManager::initHistogramParticles(){
  int rootDepth = 0;
  
  sortingRoot = new Node<NodeDescriptor>(Key(1),
                         rootDepth,
                         myParticles.getVec(),
                         myNumParticles);
  /*
    The activeBins data structure keeps track of the
    active leaves (bins) iteration after iteration. 
    We begin with the root as the only active leaf. 
    When a worker is told by the master to split an
    active leaf, it partitions its particles among
    its children and contributes the number of particles
    held by each child to a reduction.
  */
  activeBins.addNewNode(sortingRoot);

  // don't access myParticles through ckvec after this
  // anyway. these must be reset before this DM starts
  // to receive submitted particles from TPs placed on it
  myNumParticles = 0;
  myParticles.length() = 0;
}

/*
  Send counts of the particles held by this PE that belong
  to each of the newly created children (i.e. children 
  of active leaves that are being partitioned) 
*/
void DataManager::sendHistogram(){
  CkCallback cb(CkIndex_DataManager::receiveHistogram(NULL),0,this->thisgroup);
  contribute(sizeof(NodeDescriptor)*activeBins.getNumCounts(),activeBins.getCounts(),NodeDescriptorReductionType,cb);
  activeBins.reset();
}

// executed on PE 0
void DataManager::receiveHistogram(CkReductionMsg *msg){
  int numRecvdBins = msg->getSize()/sizeof(NodeDescriptor);
  NodeDescriptor *descriptors = (NodeDescriptor *)msg->getData();
  CkVec<int> binsToRefine;

  /* 
    Reserve enough space so that all active leaves 
    can be partitioned, if necessary
  */
  binsToRefine.reserve(numRecvdBins);
  binsToRefine.length() = 0;

  int particlesHistogrammed = 0;
  /*
    The newly created children of the active leaves
    are now the active leaves themselves. 
  */
  CkVec<pair<Node<NodeDescriptor>*,bool> > *active = activeBins.getActive();
  CkAssert(numRecvdBins == active->length());

  // Check which new active leaves need to be partitioned
  for(int i = 0; i < numRecvdBins; i++){
    if(descriptors[i].numParticles > (Real)(DECOMP_TOLERANCE*globalParams.ppc)){
      // Need to refine this leaf (partition)
      binsToRefine.push_back(i);
      // By refining this node, we will remove one tree piece
      // and add BRANCH_FACTOR in its place.
      numTreePieces += (BRANCH_FACTOR-1);
      // Fail if not enough tree pieces are provided
      if(numTreePieces > globalParams.numTreePieces){
        CkPrintf("have %d treepieces need %d\n",globalParams.numTreePieces,numTreePieces);
        CkAbort("Need more tree pieces!\n");
      }
    }
    else{
      // This node will NOT be refined further; save its particulars
      Node<NodeDescriptor> *nd = (*active)[i].first;
      nd->data = descriptors[i];
    }

    particlesHistogrammed += descriptors[i].numParticles;
  }

  // Do any active leaves need to be partitioned?
  if(binsToRefine.size()) {
    // Yes, tell workers which ones  
    thisProxy.receiveSplitters(binsToRefine);
    decompIterations++;
  }
  else{
    // No more leaves to refine; send out particles to tree pieces
    CkPrintf("[0] decomp done after %d iterations used treepieces %d\n", decompIterations, numTreePieces);
    decompIterations = 0;
    
    /*
      As a result of the decomposition, each tree piece has a contiguous 
      range of particles (keys). These are to be broadcast to the workers
      becaue they are needed to construct the local tree on each PE. 
    */
    keyRanges = new Key[numTreePieces*2];
    /*
      Here the master is also worker 0. It holds some particles 
      that are to be sent to the appropriate tree pieces. These
      are flushed now.
    */
    flushParticles();
    // PE 0 sets ranges in sendParticlesToTreePiece, which is
    // called by ParticleFlushWorker in flushParticles()
    haveRanges = true;
    // Find out how many tree pieces are hosted on this PE
    senseTreePieces();

    int numKeys = numTreePieces*2;

    /*
      Tell all PEs that there are no remaining active leaves,
      i.e. we have obtained a partitioning of particles on to
      tree pieces such that each tree piece gets no more than 
      a threshold (ppc) number of particles.

      Also, give the workers the range of particles held by 
      each tree piece. The worker PEs will use this information
      to find which portions of the tree are local to the PE,
      and which ones are remote.
    */
    RangeMsg *rmsg = new (numKeys) RangeMsg;
    rmsg->numTreePieces = numTreePieces;
    memcpy(rmsg->keys,keyRanges,sizeof(Key)*numKeys);
    thisProxy.sendParticles(rmsg);
  }

  delete msg;
}

/*
  As a result of the decomposition procedure, the
  particles held by each PE have been partitioned 
  among the tree pieces. It is now time to send these
  particles to their respective tree pieces.

  We do this by traversing the tree that
  was constructed during the decompostion by the 
  activeBins data structure.
*/
void DataManager::flushParticles(){
  ParticleFlushWorker pfw(this);
  scaffoldTrav.preorderTraversal(sortingRoot,&pfw);

  int numUsefulTreePieces = pfw.getNumLeaves(); 
  for(int i = numUsefulTreePieces; i < globalParams.numTreePieces; i++){
    treePieceProxy[i].receiveParticles();
  }

  // done with sorting tree; delete
  FreeTreeWorker<NodeDescriptor> freeWorker;
  scaffoldTrav.postorderTraversal(sortingRoot,&freeWorker);

  delete sortingRoot;
  sortingRoot = NULL;

}

/*
  The master communicates the active leaves to split
  to the workers through this method. The activeBins
  data structure processes the list of leaves to refine,
  creates new children for them and obtains counts
  of the number of particles under each child. These
  children are then made the active leaves for the new
  iteration and the counts are contributed to a reduction
  in sendHistogram.
*/
void DataManager::receiveSplitters(CkVec<int> splitBins) {

  // Process bins to refine
  activeBins.processRefine(splitBins.getVec(), splitBins.size());

  // Send counts of particles in active leaves to master PE
  sendHistogram();
}

/*
  Send the particles that you are holding for tree piece 'tp' to it.
*/
void DataManager::sendParticlesToTreePiece(Node<NodeDescriptor> *nd, int tp) {
  CkAssert(nd->getNumChildren() == 0);
  int np = nd->getNumParticles();

  if(np > 0){
    ParticleMsg *msg = new (np,0) ParticleMsg;
    memcpy(msg->part, nd->getParticles(), sizeof(Particle)*np);
    msg->numParticles = np;
    treePieceProxy[tp].receiveParticles(msg);
  }
  else{
    treePieceProxy[tp].receiveParticles();
  }

  /* 
    Only PE 0 (the master) has the correct ranges, 
    since it receives reduced data from all workers
  */
  if(CkMyPe() == 0){
    // Sanity checks
    if(nd->data.numParticles > 0){
      CkAssert(nd->data.smallestKey <= nd->data.largestKey);
    } else {
      CkAssert(nd->data.smallestKey == nd->data.largestKey);
    }

    keyRanges[(tp<<1)] = nd->data.smallestKey;
    keyRanges[(tp<<1)+1] = nd->data.largestKey;

  }
}

/*
  This method is invoked on workers by the master, telling
  them that it is now OK to send the particles they are
  holding to the tree pieces that they are meant for.

  It also communicates the ranges of particles held  by each
  tree piece to every PE. 
*/
void DataManager::sendParticles(RangeMsg *msg){
  if(CkMyPe() != 0){
    // Save tree piece particle ranges
    numTreePieces = msg->numTreePieces;
    // Save tree piece particle ranges
    keyRanges = msg->keys;
    haveRanges = true;
    rangeMsg = msg;
    // Flush particles to their owner tree pieces
    flushParticles();

    // Obtain the number of tree pieces that are hosted on this PE
    senseTreePieces();
  }
  else{
    /* 
      If this is worker 0, it is also the master, so it must
      have done the above at the end of receiveHistogram.
    */
    CkAssert(numTreePieces == msg->numTreePieces);
    CkAssert(haveRanges);
    delete msg;
  }
}

/*
  How many tree pieces are hosted on this PE? 
  This tells the PE how many tree pieces it should
  expect particle contributions from before starting
  to build the local tree.
*/
void DataManager::senseTreePieces(){
  localTreePieces.reset();
  CkLocMgr *mgr = treePieceProxy.ckLocMgr();
  mgr->iterate(localTreePieces);
  numLocalTreePieces = localTreePieces.count;
  /* 
    It could be that all tree pieces on this PE received their
    particles and submitted them to the DM before it received the 
    "sendParticles" message. If this is the case, proceed to tree building.
  */
  if(submittedParticles.length() == numLocalTreePieces) processSubmittedParticles();
}

/*
  This is a normal C++ function called by tree pieces hosted
  on this PE. Once a tree piece has received all the particles meant
  for it, it submits them to the DM. The DM, in turn, collects particles
  from all the tree pieces on its PE and constructs a local tree
  from them. All nodes and particles within this PE-level tree are
  then visible to all the tree pieces on the PE.
*/
void DataManager::submitParticles(CkVec<ParticleMsg*> *vec, int numParticles, TreePiece * tp, Key smallestKey, Key largestKey){ 
  submittedParticles.push_back(TreePieceDescriptor(vec,numParticles,tp,tp->getIndex(),smallestKey,largestKey));
  myNumParticles += numParticles;
  if(submittedParticles.length() == numLocalTreePieces && haveRanges){
    processSubmittedParticles();
  }
}

/*
  Once the DM has collected particles from all the tree pieces hosted
  on this PE, it copies these particles into a contiguous buffer,
  sorts them and partitions them in-place in much the same way as 
  was done during domain decomposition. The result of this partitioning
  is a local tree whose leaves are buckets containing particles on
  the PE. The nodes of this tree are of six types:

  1. Bucket: leaf containing local particles
  2. EmptyBucket: an empty bucket
  3. Internal: a node whose every child is either Internal or Bucket or EmptyBucket

  4. RemoteBucket: leaf that is local to some other PE
  5. EmptyRemoteBucket: empty RemoteBucket
  6. Remote: a node whose entire subtree is non-local to this PE

  7. Boundary: a node that is none of the above, i.e. is "shared" between several
     PEs.
*/
void DataManager::processSubmittedParticles(){
  int offset = 0;

  submittedParticles.quickSort();
  
  myParticles.resize(myNumParticles);

  for(int i = 0; i < submittedParticles.length(); i++){
    TreePieceDescriptor &descr = submittedParticles[i];
    CkVec<ParticleMsg*> *vec = descr.vec;
    for(int j = 0; j < vec->length(); j++){
      ParticleMsg *msg = (*vec)[j];
      memcpy(myParticles.getVec()+offset,msg->part,sizeof(Particle)*msg->numParticles);
      offset += msg->numParticles;
      delete msg;
    }
  }

  myParticles.quickSort();

  buildTree();
  // add dummy tree piece whose index is larger than
  // that of all others. this is required to mark the
  // boundary of nodes/particles owned by this PE.
  submittedParticles.push_back(TreePieceDescriptor(globalParams.numTreePieces));

  // makeMoments also sends out requests for moments 
  // of remote nodes
  makeMoments();

  doneTreeBuild = true;

  // are all particles local to this PE? 
  if(root != NULL && root->getType() == Internal){
    passMomentsUpward(root);
  }

  flushMomentRequests();
}

/*
 * Build the tree: continue expanding nodes starting from the 
 * root until there are either only empty buckets, remote nodes
 * or buckets at the leaves of the tree.
 */
void DataManager::buildTree(){

  int rootDepth = 0;
  root = new Node<ForceData>(Key(1),rootDepth,myParticles.getVec(),myNumParticles);
  root->setOwners(0,numTreePieces-1);
  nodeTable[Key(1)] = root;
  // No need to construct tree if there are no particles on this PE
  if(myNumParticles == 0){
    return;
  }

  OwnershipActiveBinInfo<ForceData> abi(keyRanges);
  abi.addNewNode(root);
  int numFatNodes = 1;

  int limit = ((Real)globalParams.ppb*BUCKET_TOLERANCE);

  CkVec<int> refines;

  while(numFatNodes > 0){
    abi.reset();
    refines.length() = 0;

    CkVec<std::pair<Node<ForceData>*,bool> > *active = abi.getActive();

    for(int i = 0; i < active->length(); i++){
      Node<ForceData> *node = (*active)[i].first;
      if(node->getNumParticles() > 0 && ((node->getOwnerEnd() > node->getOwnerStart()) || 
         (node->getNumParticles() > limit))){
        refines.push_back(i);
      }
    }

    abi.processRefine(refines.getVec(), refines.length());
    numFatNodes = abi.getNumCounts();
  }

}

/*
 * Compute the moments of the nodes internal to this PE's tree.
 * This also sends out requests for moments of remote nodes.
 */
void DataManager::makeMoments(){
  if(root == NULL) return;

  // Do a postorder traversal, since we must process children before
  // parents
  MomentsWorker mw(submittedParticles, nodeTable, localTPRoots, myBuckets);
  fillTrav.postorderTraversal(root,&mw);
}

Node<ForceData> *DataManager::lookupNode(Key k){
  map<Key,Node<ForceData>*>::iterator it;
  it = nodeTable.find(k);
  if(it == nodeTable.end()) return NULL;
  else return it->second;
}

/*
 * Request from a remote PE for some moments held by me. I need
 * not be the sole owner of such a node, merely one in the range
 * of collective owners.
 */
void DataManager::requestMoments(Key k, int replyTo){
  pendingMoments[k].push_back(replyTo);

  // I can respond only when I have built my tree.
  if(doneTreeBuild){    
    Node<ForceData> *node = lookupNode(k);
    if(node == NULL){
      CkPrintf("(%d) recvd request from %d for moments %lu\n", CkMyPe(), replyTo, k);
      CkAbort("bad request\n");
    }
    bool ready = node->allChildrenMomentsReady();

    // If the moments of this node are ready, we must
    // respond to all requestors of the node, and check
    // whether parent is ready, and whether it has been
    // requested by someone
    if(ready){
      map<Key,CkVec<int> >::iterator it = pendingMoments.find(k);
      CkVec<int> &requestors = it->second;
      respondToMomentsRequest(node,requestors);
      pendingMoments.erase(it);
    }
  }
}

/*
 * Flush moments after tree has been constructed. There might 
 * be several requests for each of the nodes in my tree.
 */
void DataManager::flushMomentRequests(){
  CkAssert(doneTreeBuild);
  map<Key,CkVec<int> >::iterator it;
  for(it = pendingMoments.begin(); it != pendingMoments.end();){
    Key k = it->first;
    Node<ForceData> *node = lookupNode(k);
    CkAssert(node != NULL);
    if(node->allChildrenMomentsReady()){
      CkVec<int> &requestors = it->second;
      respondToMomentsRequest(node,requestors);
      map<Key,CkVec<int> >::iterator kill = it;
      ++it;
      pendingMoments.erase(kill);
    }
    else{
      ++it;
    }
  }
}

// Helper function called from above.
void DataManager::respondToMomentsRequest(Node<ForceData> *node, CkVec<int> &replyTo){
  MomentsExchangeStruct moments = *node;
  for(int i = 0; i < replyTo.length(); i++){
    thisProxy[replyTo[i]].receiveMoments(moments);
  }
  replyTo.length() = 0;
}

// Called by a DM responding to my request for moments.
void DataManager::receiveMoments(MomentsExchangeStruct moments){
  Node<ForceData> *node = lookupNode(moments.key);
  CkAssert(node != NULL);

  // update moments of leaf and pass these on 
  // to parent recursively; if there are requests
  // for these nodes, respond to them
  updateLeafMoments(node,moments);
}

// Update the moments of the remote node that we just received
// the reply for, and pass this contribution up the tree.
void DataManager::updateLeafMoments(Node<ForceData> *node, MomentsExchangeStruct &data){
  copyMomentsToNode(node,data);
  passMomentsUpward(node);
}

// Pass moment contributions upward.
void DataManager::passMomentsUpward(Node<ForceData> *node){
  // Has anyone requested this node? If so, respond to 
  // requestor, since I have just calculated the moments
  // for this node.
  map<Key,CkVec<int> >::iterator it = pendingMoments.find(node->getKey());
  if(it != pendingMoments.end()){
    CkVec<int> &requestors = it->second;
    respondToMomentsRequest(node,requestors);
    pendingMoments.erase(it);
  }

  // Pass moments contribution up to parent and
  // check whether it is ready
  Node<ForceData> *parent = node->getParent();
  if(parent == NULL){
    CkAssert(node->getKey() == Key(1));
    treeReady();
  }else{
    parent->childMomentsReady();
    if(parent->allChildrenMomentsReady()){
      parent->getMomentsFromChildren();
      passMomentsUpward(parent);
    }
  }
}


void DataManager::treeReady(){
  treeMomentsReady = true;
  // Until my tree has been constructed, I cannot respond
  // to requests for remote data from processors that have
  // already begun their traversals.
  flushBufferedRemoteDataRequests();
  startTraversal();
}

// Flush all buffered requests for remote data.
void DataManager::flushBufferedRemoteDataRequests(){
  CkAssert(treeMomentsReady);
  for(int i = 0; i < bufferedNodeRequests.length(); i++){
    requestNode(bufferedNodeRequests[i]);
  }
  for(int i = 0; i < bufferedParticleRequests.length(); i++){
    requestParticles(bufferedParticleRequests[i]);
  }
  bufferedNodeRequests.length() = 0;
  bufferedParticleRequests.length() = 0;
}

// Helper function used in binary search
bool CompareNodePtrToKey(void *a, Key k){
  Node<ForceData> *node = *((Node<ForceData>**)a);
  return (Node<ForceData>::getParticleLevelKey(node) >= k);
}

// For each TreePiece hosted by this PE, start the
// gravity computation traversal of the distributed
// binary tree.
void DataManager::startTraversal(){
  // Tell Charm++ that useful work is now beginning
  LBTurnInstrumentOn();
  Node<ForceData> **bucketPtrs = myBuckets.getVec();
  submittedParticles[0].bucketStartIdx = 0;
  int start = 0;
  int end = myBuckets.length();

  // Find the buckets of particles assigned to each TreePiece.
  // Use the key range each TreePiece to decide which of my
  // buckets it should do the traversal for
  if(end > 0){
    for(int i = 0; i < numLocalTreePieces-1; i++){
      TreePieceDescriptor &descr = submittedParticles[i];
      // Calculate the range of buckets to assign to TreePiece
      int bucketIdx = binary_search_ge<Node<ForceData>*>(descr.largestKey,bucketPtrs,start,end,CompareNodePtrToKey);
      descr.bucketEndIdx = bucketIdx;
      int tpIndex = descr.owner->getIndex();
      // Setup TreePiece 
      descr.owner->prepare(root,localTPRoots[tpIndex],myBuckets.getVec(),descr.bucketStartIdx,descr.bucketEndIdx);
      // Tell it to start traversal
      treePieceProxy[tpIndex].startTraversal();
      submittedParticles[i+1].bucketStartIdx = bucketIdx;
      start = bucketIdx;
    }
    TreePieceDescriptor &descr = submittedParticles[numLocalTreePieces-1];
    descr.bucketEndIdx = myBuckets.length();
    int tpIndex = descr.owner->getIndex();
    descr.owner->prepare(root,localTPRoots[tpIndex],myBuckets.getVec(),descr.bucketStartIdx,descr.bucketEndIdx);
    treePieceProxy[tpIndex].startTraversal();
  }
  else if(numLocalTreePieces > 0){
    // There are no particles on this TreePiece for traversal
    for(int i = 0; i < numLocalTreePieces; i++){
      TreePieceDescriptor &descr = submittedParticles[i];
      descr.owner->prepare(root,NULL,myBuckets.getVec(),0,0);
      int tpIndex = descr.owner->getIndex();
      treePieceProxy[tpIndex].startTraversal();
    }
  }
  else{
    // This PE has no TreePieces on it at all
    finishIteration();
  }
}

/*
 *
 * Methods for serving requests for nodes and particles, as well as
 * to merge requests for remote data from all TreePieces on PE.
 *
 */

// A TreePiece on this PE has requested some remote particles.
void DataManager::requestParticles(Node<ForceData> *leaf, CutoffWorker<ForceData> *worker, State *state, Traversal<ForceData> *traversal){
  Key key = leaf->getKey();
  Request &request = particleRequestTable[key];
  // Has the request already been sent?
  if(!request.sent){
    partReqs.incrRequests();

    if(leaf->isCached()) request.parentCached = true;
    else request.parentCached = false;

    // If not, send out request
    CkAssert(leaf->getOwnerStart() == leaf->getOwnerEnd());
    int owner = leaf->getOwnerStart();
    treePieceProxy[owner].requestParticles( std::make_pair(key, CkMyPe()) );
    request.sent = true;
    
    request.parent = leaf;
  }
  // Record the fact that another TreePiece has requested these particles
  request.requestors.push_back(Requestor(worker,state,traversal,worker->getContext()));
  partReqs.incrDeliveries();
}

// This is invoked on the holder of the particles that were
// requested in the previous method. 
void DataManager::requestParticles(std::pair<Key, int> request) {
  // Check whether this DM is ready to serve
  // requests for its data.
  if(!treeMomentsReady){
    bufferedParticleRequests.push_back(request);
    return;
  }

  // Search for requested key
  map<Key,Node<ForceData>*>::iterator it = nodeTable.find(request.first);
  CkAssert(it != nodeTable.end());
  Node<ForceData> *bucket = it->second;
  CkAssert(bucket->getType() == Bucket);

  Particle *data = bucket->getParticles();
  int np = bucket->getNumParticles();

  ParticleReplyMsg *pmsg = new (np,NUM_PRIORITY_BITS) ParticleReplyMsg;
  *(int *)CkPriorityPtr(pmsg) = RECV_PARTICLES_PRIORITY;
  CkSetQueueing(pmsg,CK_QUEUEING_IFIFO);

  // Copy only essential parts of data (leave acceleration, velocity behind)
  pmsg->key = request.first;
  pmsg->np = np;
  for(int i = 0; i < np; i++){
    pmsg->data[i] = data[i];
  }

  // respond to request
  thisProxy[request.second].recvParticles(pmsg);
}

// Similar methods for node requests
void DataManager::requestNode(Node<ForceData> *leaf, CutoffWorker<ForceData> *worker, State *state, Traversal<ForceData> *traversal){
  Key key = leaf->getKey();
  Request &request = nodeRequestTable[key];
  if(!request.sent){
    nodeReqs.incrRequests();

    if(leaf->isCached()) request.parentCached = true;
    else request.parentCached = false;

    int numOwners = leaf->getOwnerEnd()-leaf->getOwnerStart()+1;
    int requestOwner = leaf->getOwnerStart()+(rand()%numOwners);
    treePieceProxy[requestOwner].requestNode( std::make_pair(key, CkMyPe()) );
    request.sent = true;
    request.parent = leaf;
  }
  request.requestors.push_back(Requestor(worker,state,traversal,worker->getContext()));
  nodeReqs.incrDeliveries();
}

// Invoked on holder of node. 
void DataManager::requestNode(std::pair<Key, int> request) {
  if(!treeMomentsReady){
    bufferedNodeRequests.push_back(request);
    return;
  }

  RRDEBUG("(%d) REPLY node %lu to %d\n", CkMyPe(), request.first, request.second);


  // Locate the node in your tree
  map<Key,Node<ForceData>*>::iterator it = nodeTable.find(request.first);
  CkAssert(it != nodeTable.end());
  Node<ForceData> *node = it->second;

  CkAssert(node->getNumChildren() > 0);

  // Calculate the size of the subtree under the node
  // that is to be communicated 
  TreeSizeWorker tsz(node->getDepth()+globalParams.cacheLineSize);
  fillTrav.topDownTraversal_local(node,&tsz);

  int nn = tsz.getNumNodes();

  NodeReplyMsg *nmsg = new (nn,NUM_PRIORITY_BITS) NodeReplyMsg;
  *(int *)CkPriorityPtr(nmsg) = RECV_NODE_PRIORITY;
  CkSetQueueing(nmsg,CK_QUEUEING_IFIFO);

  nmsg->key = request.first;
  nmsg->nn = nn;

  // Serialize the tree data structure into the allocated message buffer
  Node<ForceData> *emptyBuf = nmsg->data;
  node->serialize(NULL,emptyBuf,globalParams.cacheLineSize);
  CkAssert(emptyBuf == nmsg->data+nn);

  // Respond to request
  thisProxy[request.second].recvNode(nmsg);
}

// Receive particles that you had requested some time previously
void DataManager::recvParticles(ParticleReplyMsg *msg){
  map<Key,Request>::iterator it = particleRequestTable.find(msg->key);
  CkAssert(it != particleRequestTable.end());

  // Make sure that a request was sent!
  Request &req = it->second;
  CkAssert(req.requestors.length() > 0);
  CkAssert(req.sent);
  CkAssert(req.msg == NULL);

  // Keep track of message: will need pointer when
  // freeing cached data
  req.msg = msg;
  req.data = msg->data;
  
  // attach particles to bucket in tree 
  Node<ForceData> *leaf = req.parent;
  CkAssert(leaf != NULL);
  CkAssert(leaf->getType() == RemoteBucket);
  leaf->setParticles((Particle *)msg->data,msg->np);

  // Do some book-keeping
  partReqs.decrRequests();
  partReqs.decrDeliveries(req.requestors.length());
  // Deliver particles to requestors
  req.deliverParticles(msg->np);
}

// Receive nodes that were requested some time previously
void DataManager::recvNode(NodeReplyMsg *msg){
  map<Key,Request>::iterator it = nodeRequestTable.find(msg->key);
  CkAssert(it != nodeRequestTable.end());

  Request &req = it->second;
  CkAssert(req.requestors.length() > 0);
  CkAssert(req.sent);
  CkAssert(req.msg == NULL);

  req.msg = msg;
  req.data = msg->data;
  
  // attach recvd subtree to appropriate point in local tree
  Node<ForceData> *node = req.parent;
  CkAssert(node != NULL);
  
  node->deserialize(msg->data, msg->nn);

  // Book-keeping
  nodeReqs.decrRequests();
  nodeReqs.decrDeliveries(req.requestors.length());

  // Deliver subtree to requestors
  req.deliverNode();
}

// A TreePiece on this PE has finished its traversals
void DataManager::traversalsDone(CmiUInt8 pnInter, CmiUInt8 ppInter, CmiUInt8 openCrit)
{
  // Calculate some statistics
  numTreePiecesDoneTraversals++;
  numInteractions[0] += pnInter;
  numInteractions[1] += ppInter;
  numInteractions[2] += openCrit;
  if(numTreePiecesDoneTraversals == numLocalTreePieces){
    finishIteration();
  }
}

// All TreePieces on this PE have finished; time to make a contribution
// to soft barrier, after which we can advance particles
void DataManager::finishIteration(){
  // can't advance particles here, because other PEs 
  // might not have finished their traversals yet, 
  // and therefore might need my particles

  CkAssert(nodeReqs.test());
  CkAssert(partReqs.test());

  DtReductionStruct dtred;
  dtred.pnInteractions = numInteractions[0];
  dtred.ppInteractions = numInteractions[1];
  dtred.openCrit = numInteractions[2];

  CkCallback cb(CkIndex_DataManager::advance(NULL),thisProxy);
  contribute(sizeof(DtReductionStruct),&dtred,DtReductionType,cb);

}

// Update velocities and positions of particles
void DataManager::advance(CkReductionMsg *msg){

  DtReductionStruct *dtred = (DtReductionStruct *)(msg->getData());
  myBox.reset();
  // Moving particles may cause the bounding box to change
  kickDriftKick(myBox.box,myBox.energy);

  Real pad = 0.00001;
  myBox.expand(pad);
  myBox.numParticles = myNumParticles;

  if(CkMyPe() == 0){
    CkPrintf("[STATS] node inter %lu part inter %lu open crit %lu dt %f\n", dtred->pnInteractions, dtred->ppInteractions, dtred->openCrit, globalParams.dtime);
  }

  iteration++;
  CkCallback cb;
  if(iteration == globalParams.iterations){
    // We are done with the simulation!
    cb = CkCallback(CkIndex_Main::niceExit(),mainProxy);
    contribute(0,0,CkReduction::sum_int,cb);
  }
  else if(iteration % globalParams.balancePeriod == 0){
    // If it is a load-balancing iteration, tell all TreePieces
    // on this PE to start load balancing
    if(CkMyPe() == 0) CkPrintf("(%d) INITIATE LB\n", CkMyPe()); 
    for(int i = 0; i < submittedParticles.length()-1; i++){
      TreePiece *tp = submittedParticles[i].owner;
      tp->startlb();
    }
  }
  else{
    init();
    // Can begin the next iteration as soon as the new bounding box
    // of all particles has been obtained
    cb = CkCallback(CkIndex_DataManager::recvUnivBoundingBox(NULL),thisProxy);
    contribute(sizeof(BoundingBox),&myBox,BoundingBoxGrowReductionType,cb);
  }
  
  delete msg;
}

// Starting point for new iteration
void DataManager::recvUnivBoundingBox(CkReductionMsg *msg){
  BoundingBox &univBB = *((BoundingBox *)msg->getData());
  decompose(univBB);
  delete msg;
}

// It is now safe to free all cached data, as well as delete
// your local tree (it will be reconstructed in the next iteration)
// since no one can make requests for your data anymore
void DataManager::freeCachedData(){
  map<Key,Request>::iterator it;


  for(it = particleRequestTable.begin(); it != particleRequestTable.end(); it++){
    Request &request = it->second;
    CkAssert(request.sent);
    CkAssert(request.data != NULL);
    CkAssert(request.requestors.length() == 0);
    CkAssert(request.msg != NULL);
    
    // no need to set the particles of a cached
    // bucket to NULL: we will delete the bucket
    // anyway
    if(!request.parentCached){
      request.parent->setParticles(NULL,0);
    }

    delete (ParticleReplyMsg *)(request.msg);
  }

  for(it = nodeRequestTable.begin(); it != nodeRequestTable.end(); it++){
    Request &request = it->second;
    CkAssert(request.sent);
    CkAssert(request.data != NULL);
    CkAssert(request.requestors.length() == 0);
    CkAssert(request.msg != NULL);

    // tell the root of the nodes in this 
    // fetched entry that its children don't
    // exist anymore
    // cached parents may be deleted before
    // their children, so we don't set their
    // children
    if(!request.parentCached){
      request.parent->setChildren(NULL,0);
    }

    delete (NodeReplyMsg *)(request.msg);
  }

  nodeRequestTable.clear();
  particleRequestTable.clear();
}

void DataManager::freeTree(){
  if(root != NULL){
    FreeTreeWorker<ForceData> freeWorker;
    fillTrav.postorderTraversal(root,&freeWorker); 
    delete root;
    root = NULL;
  }
}

void DataManager::kickDriftKick(OrientedBox<Real> &box, Real &energy){
  Vector3D<Real> dv;

  Particle *pstart = myParticles.getVec();
  Particle *pend = myParticles.getVec()+myNumParticles;

  Real particleEnergy;
  Real particleKinetic;
  Real particlePotential;
  for(Particle *p = pstart; p != pend; p++){
    particlePotential = p->mass*p->potential;
    particleKinetic = 0.5*p->mass*p->velocity.lengthSquared();
    particleEnergy = particlePotential+particleKinetic;
    energy += particleEnergy;

    // kick
    p->velocity += globalParams.dthf*p->acceleration;
    // drift
    p->position += globalParams.dtime*p->velocity;
    // kick
    p->velocity += globalParams.dthf*p->acceleration;
    
    box.grow(p->position);

    p->acceleration = Vector3D<Real>(0.0);
    p->potential = 0.0;

  }
}

void DataManager::init(){
  LBTurnInstrumentOff();
  CkAssert(pendingMoments.empty());
  // safe to reset here, since all tree pieces 
  // must have finished iteration
  freeCachedData();

  decompIterations = 0;

  haveRanges = false;
  numLocalTreePieces = -1;
  myBuckets.length() = 0;
  doneTreeBuild = false;
  treeMomentsReady = false;
  numTreePiecesDoneTraversals = 0;

  firstSplitterRound = true;
  freeTree();
  nodeTable.clear();
  localTPRoots.clear();

  CkAssert(activeBins.getNumCounts() == 0);

  if(CkMyPe() == 0 && keyRanges != NULL){
    delete[] keyRanges;
    keyRanges = NULL;
  }
  else if(CkMyPe() != 0){
    delete rangeMsg;
    rangeMsg = NULL;
  }


  numInteractions[0] = 0;
  numInteractions[1] = 0;
  numInteractions[2] = 0;
  submittedParticles.length() = 0;
}

void DataManager::resumeFromLB(){
  /* we delay the freeing of data structures to this point
   * because we want the tree pieces to be able to use the
   * tree, and in particular their roots for load balancing
   * */
  init();
  CkCallback cb(CkIndex_DataManager::recvUnivBoundingBox(NULL),thisProxy);
  contribute(sizeof(BoundingBox),&myBox,BoundingBoxGrowReductionType,cb);
}

#include "Traversal_defs.h"

